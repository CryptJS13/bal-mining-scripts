{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "REALTIME_ESTIMATOR = False\n",
    "# set the window of blocks, will be overwritten if REALTIME_ESTIMATOR == True\n",
    "week_1_start_ts = 1590969600\n",
    "WEEK = 57\n",
    "week_end_timestamp = week_1_start_ts + WEEK * 7 * 24 * 60 * 60\n",
    "week_start_timestamp = week_end_timestamp - 7 * 24 * 60 * 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "incentivized_polygon_pools = [\n",
    "    '0x0297e37f1873d2dab4487aa67cd56b58e2f27875000100000000000000000002',\n",
    "    '0xce66904b68f1f070332cbc631de7ee98b650b499000100000000000000000009',\n",
    "    '0x36128d5436d2d70cab39c9af9cce146c38554ff0000100000000000000000008',\n",
    "    '0xf461f2240b66d55dcf9059e26c022160c06863bf000100000000000000000006',\n",
    "    '0x32fc95287b14eaef3afa92cccc48c285ee3a280a000100000000000000000005'\n",
    "]\n",
    "incentivized_polygon_pools_addresses = [\n",
    "    p[:42] for p in incentivized_polygon_pools\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Consider installing rusty-rlp to improve pyrlp performance with a rust based backend\n",
      "<ipython-input-3-26c0cad345e5>:10: UserWarning: Running realtime estimator\n",
      "  warnings.warn('Running realtime estimator')\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud import bigquery_storage\n",
    "import warnings\n",
    "import time\n",
    "from web3 import Web3\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "if REALTIME_ESTIMATOR:\n",
    "    warnings.warn('Running realtime estimator')\n",
    "    \n",
    "    from urllib.request import urlopen\n",
    "    import json\n",
    "    url = 'https://ipfs.fleek.co/ipns/balancer-team-bucket.storage.fleek.co/balancer-claim/snapshot'\n",
    "    jsonurl = urlopen(url)\n",
    "    claims = json.loads(jsonurl.read())\n",
    "    claimable_weeks = [20+int(w) for w in claims.keys()]\n",
    "    most_recent_week = max(claimable_weeks)\n",
    "    # delete the estimates for the most recent published week, since now there's an official value available on IPFS\n",
    "    project_id = os.environ['GCP_PROJECT']\n",
    "    sql = f'''\n",
    "        DELETE FROM {project_id}.bal_mining_estimates.lp_estimates_tmp\n",
    "        WHERE week = {most_recent_week}\n",
    "    '''\n",
    "    client = bigquery.Client()\n",
    "    query = client.query(sql)\n",
    "    query.result()\n",
    "    \n",
    "    \n",
    "    from datetime import datetime\n",
    "    week_1_start = '01/06/2020 00:00:00 UTC'\n",
    "    week_1_start = datetime.strptime(week_1_start, '%d/%m/%Y %H:%M:%S %Z')\n",
    "    WEEK = int(1 + (datetime.utcnow() - week_1_start).days/7)  # this is what week we're actually in\n",
    "    week_end_timestamp = week_1_start_ts + WEEK * 7 * 24 * 60 * 60\n",
    "    week_start_timestamp = week_end_timestamp - 7 * 24 * 60 * 60\n",
    "    week_end_timestamp = int(datetime.utcnow().timestamp())\n",
    "    week_passed = (week_end_timestamp - week_start_timestamp)/(7*24*3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAL mined on V2 on week 57: 145000\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "import json\n",
    "# V2 allocation\n",
    "V2_LM_ALLOCATION_URL = 'https://raw.githubusercontent.com/balancer-labs/frontend-v2/master/src/lib/utils/liquidityMining/LiquidityMiningV2.json'\n",
    "jsonurl = urlopen(V2_LM_ALLOCATION_URL)\n",
    "try:\n",
    "    V2_ALLOCATION_THIS_WEEK = json.loads(jsonurl.read())[f'week_{WEEK}']['tiers']\n",
    "except KeyError:\n",
    "    V2_ALLOCATION_THIS_WEEK = {}\n",
    "V2_MINING_POOLS = {}\n",
    "for tier in V2_ALLOCATION_THIS_WEEK.values():\n",
    "    for pool in tier['slots'].values():\n",
    "        pool_address = pool[:42].lower()\n",
    "        V2_MINING_POOLS[pool_address] = V2_MINING_POOLS.get(pool_address,0) + tier['BAL']\n",
    "BAL_MINED_ON_V2 = sum(V2_MINING_POOLS.values())\n",
    "print(f'BAL mined on V2 on week {WEEK}: {BAL_MINED_ON_V2}')\n",
    "\n",
    "CLAIM_PRECISION = 18 # leave out of results addresses that mined less than CLAIM_THRESHOLD BAL\n",
    "CLAIM_THRESHOLD = 10**(-CLAIM_PRECISION)\n",
    "reports_dir = f'reports/{WEEK}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get addresses that redirect\n",
    "if REALTIME_ESTIMATOR:\n",
    "    url = 'https://raw.githubusercontent.com/balancer-labs/bal-mining-scripts/master/config/redirect.json'\n",
    "    jsonurl = urlopen(url)\n",
    "    redirects = json.loads(jsonurl.read())\n",
    "else:\n",
    "    redirects = json.load(open('config/redirect.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V2 Liquidity Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------ethereum-----------------\n",
      "2021-06-29 19:16:33 - Querying Bigquery for the V2 LPs...\n",
      "2021-06-29 19:17:15 - Done!\n",
      "36223.60367063491 BAL mined on ethereum\n",
      "Redirect: 0 redirectors found\n",
      "-----------------polygon-----------------\n",
      "2021-06-29 19:17:15 - Querying Bigquery for the V2 LPs...\n",
      "2021-06-29 19:17:52 - Done!\n",
      "6306.133432539699 BAL mined on polygon\n",
      "Redirect: 0 redirectors found\n"
     ]
    }
   ],
   "source": [
    "def v2_liquidity_mining(week, \n",
    "                        pools_addresses_and_BAL_earned,\n",
    "                        network):\n",
    "    print(f'-----------------{network}-----------------')\n",
    "\n",
    "    network_blocks_table = {\n",
    "        'ethereum': 'bigquery-public-data.crypto_ethereum.blocks',\n",
    "        'polygon': 'public-data-finance.crypto_polygon.blocks',\n",
    "    }\n",
    "    \n",
    "    bpt_balances_table = {\n",
    "        'ethereum': 'blockchain-etl.ethereum_balancer.view_token_balances_subset',\n",
    "        'polygon': 'blockchain-etl.polygon_balancer.view_bpt_balances',\n",
    "    }\n",
    "\n",
    "    with open('src/liquidity_mining_V2.sql','r') as file:\n",
    "        sql = (\n",
    "            file\n",
    "            .read()\n",
    "            .format(\n",
    "                week, \n",
    "                '\\',\\''.join(pools_addresses_and_BAL_earned.keys()),\n",
    "                network_blocks_table[network],\n",
    "                bpt_balances_table[network]\n",
    "            )\n",
    "        )\n",
    "    # print(sql)\n",
    "\n",
    "    print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Querying Bigquery for the V2 LPs...')\n",
    "\n",
    "    client = bigquery.Client()\n",
    "    bqstorageclient = bigquery_storage.BigQueryReadClient()\n",
    "    time_weighted_share = (\n",
    "        client.query(sql)\n",
    "        .result()\n",
    "        .to_dataframe(bqstorage_client=bqstorageclient)\n",
    "    )\n",
    "    print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Done!')\n",
    "    time_weighted_share['miner'] = time_weighted_share['miner'].apply(Web3.toChecksumAddress)\n",
    "    time_weighted_share.set_index(['pool_address','miner'], inplace=True)\n",
    "\n",
    "    weekly_pools_BAL = pd.Series(pools_addresses_and_BAL_earned)\n",
    "    weekly_pools_BAL.index.name = 'pool_address'\n",
    "\n",
    "    bal_mined_v2 = time_weighted_share['share']*weekly_pools_BAL\n",
    "    if REALTIME_ESTIMATOR:\n",
    "        bal_mined_v2 *= week_passed\n",
    "\n",
    "    miner_export_v2 = bal_mined_v2.groupby('miner').sum()\n",
    "    print(f'{miner_export_v2.sum()} BAL mined on {network}')\n",
    "\n",
    "    v2_miners = pd.DataFrame(miner_export_v2).reset_index()\n",
    "    n = len(v2_miners['miner'].drop_duplicates()[v2_miners['miner'].drop_duplicates().isin(redirects.keys())])\n",
    "    print(f'Redirect: {n} redirectors found')\n",
    "    v2_miners['miner'] = v2_miners['miner'].apply(lambda x: redirects.get(x,x))\n",
    "    miner_export_v2 = v2_miners.groupby('miner').sum()[0]\n",
    "\n",
    "    if not REALTIME_ESTIMATOR:\n",
    "        filename = f'/_{network}.json'\n",
    "        (\n",
    "            miner_export_v2[miner_export_v2>=CLAIM_THRESHOLD]\n",
    "            .apply(\n",
    "                lambda x: format(\n",
    "                    x, \n",
    "                    f'.{CLAIM_PRECISION}f'\n",
    "                )\n",
    "            )\n",
    "            .to_json(reports_dir+filename, indent=4)\n",
    "        )\n",
    "\n",
    "    return miner_export_v2\n",
    "\n",
    "\n",
    "miners_ethereum = v2_liquidity_mining(\n",
    "    WEEK, \n",
    "    {k: v for k,v in V2_MINING_POOLS.items() if k not in incentivized_polygon_pools_addresses},\n",
    "    'ethereum')\n",
    "miners_polygon = v2_liquidity_mining(\n",
    "    WEEK, \n",
    "    {k: v for k,v in V2_MINING_POOLS.items() if k in incentivized_polygon_pools_addresses},\n",
    "    'polygon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "miners_all = miners_ethereum.add(miners_polygon, fill_value=0)\n",
    "if not REALTIME_ESTIMATOR:\n",
    "    filename = '/_totalsLiquidityMining.json'\n",
    "    (\n",
    "        miners_all[miners_all>=CLAIM_THRESHOLD]\n",
    "        .apply(\n",
    "            lambda x: format(\n",
    "                x, \n",
    "                f'.{CLAIM_PRECISION}f'\n",
    "            )\n",
    "        )\n",
    "        .to_json(reports_dir+filename, indent=4)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update real time estimates in GBQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:06,  6.19s/it]\n"
     ]
    }
   ],
   "source": [
    "if REALTIME_ESTIMATOR:\n",
    "    sql = f'''\n",
    "        UPDATE {project_id}.bal_mining_estimates.lp_estimates_tmp\n",
    "        SET velocity = '0'\n",
    "        WHERE week = {WEEK-1}\n",
    "    '''\n",
    "    client = bigquery.Client()\n",
    "    query = client.query(sql)\n",
    "    query.result();\n",
    "\n",
    "    # write to GBQ (LPs)\n",
    "    cur_estimate = pd.DataFrame(miners_ethereum)\n",
    "    cur_estimate.columns = ['earned']\n",
    "    cur_estimate.index.name = 'address'\n",
    "    \n",
    "    try:\n",
    "        prev_estimate = pd.read_gbq(f'select address, earned, timestamp from bal_mining_estimates.lp_estimates_tmp WHERE week = {WEEK}', \n",
    "                        project_id=os.environ['GCP_PROJECT'])\n",
    "        prev_estimate.set_index('address', inplace=True)\n",
    "        prev_estimate_timestamp = prev_estimate.iloc[0]['timestamp']\n",
    "    except:\n",
    "        prev_estimate_timestamp = 0\n",
    "    if prev_estimate_timestamp < week_start_timestamp:\n",
    "        #previous estimate is last week's; compute velocity from end_block_timestamp and start_block_timestamp\n",
    "        delta_t = (week_end_timestamp - week_start_timestamp)\n",
    "        earned = cur_estimate['earned'].astype(float)\n",
    "        cur_estimate['velocity'] = (earned/delta_t).apply(lambda x: format(x, f'.{18}f'))\n",
    "    else:\n",
    "        #compute velocity based on increase and time passed\n",
    "        delta_t = (week_end_timestamp - prev_estimate_timestamp)\n",
    "        diff_estimate = cur_estimate.join(prev_estimate, rsuffix='_prev').fillna(0)\n",
    "        cur_earned = diff_estimate['earned'].astype(float)\n",
    "        prev_earned = diff_estimate['earned_prev'].astype(float)\n",
    "        cur_estimate['velocity'] = ((cur_earned-prev_earned)/delta_t).apply(lambda x: format(x, f'.{18}f'))\n",
    "        \n",
    "    # delete this week's previous estimates\n",
    "    sql = f'''\n",
    "        DELETE FROM {project_id}.bal_mining_estimates.lp_estimates_tmp\n",
    "        WHERE week = {WEEK}\n",
    "    '''\n",
    "    client = bigquery.Client()\n",
    "    query = client.query(sql)\n",
    "    query.result();\n",
    "\n",
    "    cur_estimate['earned'] = cur_estimate['earned'].apply(lambda x: format(x, f'.{18}f'))\n",
    "    cur_estimate['timestamp'] = week_end_timestamp\n",
    "    cur_estimate['week'] = WEEK\n",
    "    cur_estimate.reset_index(inplace=True)\n",
    "    cur_estimate.to_gbq( 'bal_mining_estimates.lp_estimates_tmp', \n",
    "                        project_id=os.environ['GCP_PROJECT'], \n",
    "                        if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gas Reimbursement Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from src.bal4gas_V1 import compute_bal_for_gas as compute_bal_for_gas_V1\n",
    "from src.bal4gas_V2 import compute_bal_for_gas as compute_bal_for_gas_V2\n",
    "\n",
    "if not REALTIME_ESTIMATOR:\n",
    "    whitelist = pd.read_json(f'https://raw.githubusercontent.com/balancer-labs/assets/w{WEEK-1}/lists/eligible.json').index.values\n",
    "    gas_whitelist = pd.Series(whitelist).str.lower().tolist()\n",
    "    gas_whitelist.append('0xeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee')\n",
    "\n",
    "    \n",
    "    v1 = compute_bal_for_gas_V1(week_start_timestamp, week_end_timestamp, gas_whitelist, plot=True, verbose=True)\n",
    "\n",
    "    gas_whitelist.remove('0xeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee')\n",
    "    gas_whitelist.append('0x0000000000000000000000000000000000000000')\n",
    "    v2 = compute_bal_for_gas_V2(week_start_timestamp, week_end_timestamp, gas_whitelist, plot=True, verbose=True)\n",
    "    \n",
    "    merge = v1.append(v2)\n",
    "\n",
    "    totals_bal4gas = merge[['address','bal_reimbursement']].groupby('address').sum()['bal_reimbursement']\n",
    "    totals_bal4gas[totals_bal4gas>=CLAIM_THRESHOLD].apply(\\\n",
    "       lambda x: format(x, f'.{CLAIM_PRECISION}f')).to_json(reports_dir+'/_gasReimbursement.json',\n",
    "       indent=4)\n",
    "\n",
    "    # combine BAL from liquidity mining and gas reimbursements\n",
    "    totals = miners_ethereum.add(totals_bal4gas, fill_value=0)\n",
    "    totals[totals>=CLAIM_THRESHOLD].apply(\\\n",
    "       lambda x: format(x, f'.{CLAIM_PRECISION}f')).to_json(reports_dir+'/_totals.json',\n",
    "       indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not REALTIME_ESTIMATOR:\n",
    "    print('Final Check Totals')\n",
    "    _ethereum = pd.read_json(reports_dir+'/_ethereum.json', orient='index').sum().values[0]\n",
    "    _polygon = pd.read_json(reports_dir+'/_polygon.json', orient='index').sum().values[0]\n",
    "    _lm_both = pd.read_json(reports_dir+'/_totalsLiquidityMining.json', orient='index').sum().values[0]\n",
    "    _claim = pd.read_json(reports_dir+'/_totals.json', orient='index').sum().values[0]\n",
    "    print(f'Liquidity Mining Ethereum: {format(_ethereum, f\".{CLAIM_PRECISION}f\")}')\n",
    "    print(f'Liquidity Mining Polygon: {format(_polygon, f\".{CLAIM_PRECISION}f\")}')\n",
    "    print(f'Liquidity Mining Both: {format(_lm_both, f\".{CLAIM_PRECISION}f\")}')\n",
    "    print(f'Gas Reimbursement week {WEEK}: {format(_claim-145000, f\".{CLAIM_PRECISION}f\")}')\n",
    "    print(f'Claims: {format(_claim, f\".{CLAIM_PRECISION}f\")}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
