{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "# Google BigQuery SQL to get the blocks mined around a timestamp\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "# SELECT * FROM `bigquery-public-data.crypto_ethereum.blocks`\n",
    "# WHERE timestamp > \"2021-02-28 23:59:30\"\n",
    "# and timestamp < \"2021-03-01 00:00:30\"\n",
    "# order by timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REALTIME_ESTIMATOR = False\n",
    "# set the window of blocks, will be overwritten if REALTIME_ESTIMATOR == True\n",
    "WEEK = 40\n",
    "START_BLOCK = 11948959\n",
    "END_BLOCK = 11994473\n",
    "# we can hard code latest gov proposal if we want\n",
    "latest_gov_proposal = ''\n",
    "gov_factor = 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import warnings\n",
    "\n",
    "\n",
    "if REALTIME_ESTIMATOR:\n",
    "    warnings.warn('Running realtime estimator')\n",
    "    \n",
    "    from urllib.request import urlopen\n",
    "    import json\n",
    "    url = 'https://ipfs.fleek.co/ipns/balancer-team-bucket.storage.fleek.co/balancer-claim/snapshot'\n",
    "    jsonurl = urlopen(url)\n",
    "    claims = json.loads(jsonurl.read())\n",
    "    claimable_weeks = [20+int(w) for w in claims.keys()]\n",
    "    most_recent_week = max(claimable_weeks)\n",
    "    # delete the estimates for the most recent published week, since now there's an official value available on IPFS\n",
    "    project_id = os.environ['GCP_PROJECT']\n",
    "    sql = f'''\n",
    "        DELETE FROM {project_id}.bal_mining_estimates.pool_estimates\n",
    "        WHERE week = {most_recent_week}\n",
    "    '''\n",
    "    client = bigquery.Client()\n",
    "    query = client.query(sql)\n",
    "    query.result()\n",
    "    sql = f'''\n",
    "        DELETE FROM {project_id}.bal_mining_estimates.lp_estimates\n",
    "        WHERE week = {most_recent_week}\n",
    "    '''\n",
    "    client = bigquery.Client()\n",
    "    query = client.query(sql)\n",
    "    query.result()\n",
    "    \n",
    "    \n",
    "    from datetime import datetime\n",
    "    week_1_start = '01/06/2020 00:00:00 UTC'\n",
    "    week_1_start = datetime.strptime(week_1_start, '%d/%m/%Y %H:%M:%S %Z')\n",
    "    WEEK = int(1 + (datetime.utcnow() - week_1_start).days/7)  # this is what week we're actually in\n",
    "    \n",
    "    # get all blocks of the first hour of this week to determine the first block of the week\n",
    "    sql = '''\n",
    "    SELECT number FROM `bigquery-public-data.crypto_ethereum.blocks`\n",
    "    where timestamp >= TIMESTAMP_TRUNC(CURRENT_TIMESTAMP(), WEEK(MONDAY))\n",
    "    and timestamp <= TIMESTAMP_ADD(TIMESTAMP_TRUNC(CURRENT_TIMESTAMP(), WEEK(MONDAY)), interval 1 HOUR)\n",
    "    order by timestamp\n",
    "    limit 1\n",
    "    '''\n",
    "    results = bigquery.Client().query(sql).result()\n",
    "    for row in results:\n",
    "        START_BLOCK = row.number\n",
    "\n",
    "    sql = '''\n",
    "    SELECT MAX(number) as number FROM `bigquery-public-data.crypto_ethereum.blocks`\n",
    "    '''\n",
    "    results = bigquery.Client().query(sql).result()\n",
    "    for row in results:\n",
    "        END_BLOCK = row.number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from web3 import Web3\n",
    "import os\n",
    "web3_provider = os.environ['ENDPOINT_URL']\n",
    "w3 = Web3(Web3.WebsocketProvider(web3_provider))\n",
    "start_block_timestamp = w3.eth.getBlock(START_BLOCK).timestamp\n",
    "end_block_timestamp = w3.eth.getBlock(END_BLOCK).timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# the realtime estimator overrides the hard coded gov proposal\n",
    "if REALTIME_ESTIMATOR:\n",
    "    latest_gov_proposal = ''\n",
    "\n",
    "# if not explicitly set, try to get the latest gov proposal from the snapshot API\n",
    "if gov_factor > 1 and not latest_gov_proposal:\n",
    "    space_url = 'https://hub.snapshot.page/api/spaces/balancer'\n",
    "    members = json.loads(requests.get(space_url).content).get('members')\n",
    "\n",
    "    proposals_url = 'https://hub.snapshot.page/api/balancer/proposals'\n",
    "    proposals = pd.DataFrame(json.loads(requests.get(proposals_url).content)).transpose()\n",
    "    # only official proposals count\n",
    "    proposals = proposals[proposals['address'].isin(members)]\n",
    "    proposals['end'] = proposals['msg'].apply(lambda x: x['payload'].get('end',0))\n",
    "    # only proposals that ended before the beginning of the current week count\n",
    "    proposals = proposals[proposals['end'] < start_block_timestamp]\n",
    "    latest_gov_proposals = proposals[proposals['end']==proposals['end'].max()]\n",
    "    for i,p in latest_gov_proposals.iterrows():\n",
    "        print(f'Most recent governance proposal: {i}')\n",
    "        print(f\"Title: {p['msg']['payload'].get('name',0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of tokens eligible for liquidity mining\n",
    "if REALTIME_ESTIMATOR:\n",
    "    whitelist_df = pd.read_json('https://raw.githubusercontent.com/balancer-labs/assets/master/lists/eligible.json')\n",
    "else:\n",
    "    whitelist_df = pd.read_json(f'https://raw.githubusercontent.com/balancer-labs/assets/w{WEEK}/lists/eligible.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import json5 as json\n",
    "if REALTIME_ESTIMATOR:\n",
    "    url = 'https://raw.githubusercontent.com/balancer-labs/bal-mining-scripts/master/config/blacklisted_sharelholders.json'\n",
    "    jsonurl = urlopen(url)\n",
    "    BLACKLISTED_SHAREHOLDERS = json.loads(jsonurl.read())['address']\n",
    "\n",
    "    url = 'https://raw.githubusercontent.com/balancer-labs/bal-mining-scripts/master/config/equivalent_sets.json5'\n",
    "    jsonurl = urlopen(url)\n",
    "    EQUIVALENT_SETS = json.loads(jsonurl.read())['sets']\n",
    "else:\n",
    "    BLACKLISTED_SHAREHOLDERS = json.load(open('config/blacklisted_sharelholders.json'))['address']\n",
    "    EQUIVALENT_SETS = json.load(open('config/equivalent_sets.json5'))['sets']\n",
    "    \n",
    "BLACKLISTED_SHAREHOLDERS_lower = [x.lower() for x in BLACKLISTED_SHAREHOLDERS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAL_TOKEN = '0xba100000625a3754423978a60c9317c58a424e3D'\n",
    "SNAPSHOT_WINDOW_SIZE = 256\n",
    "CLAIM_PRECISION = 18 # leave out of results addresses that mined less than CLAIM_THRESHOLD BAL\n",
    "CLAIM_THRESHOLD = 10**(-CLAIM_PRECISION)\n",
    "WEEKLY_MINED = 145000\n",
    "LIQUIDITY_STAKING = 45000\n",
    "if REALTIME_ESTIMATOR:\n",
    "    week_passed = (end_block_timestamp - start_block_timestamp)/(7*24*3600)\n",
    "    WEEKLY_MINED = int(WEEKLY_MINED*week_passed)\n",
    "    LIQUIDITY_STAKING = int(LIQUIDITY_STAKING*week_passed)\n",
    "STAKERS_SHARE = LIQUIDITY_STAKING / WEEKLY_MINED\n",
    "reports_dir = f'reports/{WEEK}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(reports_dir):\n",
    "    os.mkdir(reports_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "flatten_list = lambda t: [item for sublist in t for item in sublist]\n",
    "\n",
    "def get_wrap_factor_of_pair(a, b):\n",
    "    a = Web3.toChecksumAddress(a)\n",
    "    b = Web3.toChecksumAddress(b)\n",
    "    for soft in EQUIVALENT_SETS:\n",
    "        for hard in soft:\n",
    "            if a in hard and b in hard:\n",
    "                return .1\n",
    "        flat = flatten_list(soft)\n",
    "        if a in flat and b in flat:\n",
    "            return .2\n",
    "    return 1\n",
    "\n",
    "\n",
    "def get_wrap_factor(tokens_weights):\n",
    "    length = len(tokens_weights)\n",
    "    shape = (length, length)\n",
    "    wf_matrix = np.zeros(shape)\n",
    "    tokens = tokens_weights.index.get_level_values('token_address')\n",
    "    for i in range(len(tokens_weights)-1):\n",
    "        for j in range(i+1, len(tokens_weights)):\n",
    "            wf_matrix[i,j] = wf_matrix[i,j] + get_wrap_factor_of_pair(tokens[i],tokens[j])\n",
    "    weights_vector = np.array(tokens_weights)\n",
    "    weights_matrix = np.outer(weights_vector,weights_vector.T)\n",
    "    try:\n",
    "        element_wise_product = weights_matrix * wf_matrix\n",
    "    except:\n",
    "        print(tokens_weights)\n",
    "        print(weights_matrix)\n",
    "        print(wf_matrix)\n",
    "        raise\n",
    "    return element_wise_product.sum()/np.triu(weights_matrix, k=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_staking_boosts_of_pair(a, b, bal_multiplier=1):\n",
    "    a = Web3.toChecksumAddress(a)\n",
    "    b = Web3.toChecksumAddress(b)\n",
    "    if a==BAL_TOKEN:\n",
    "        cap_b = whitelist_df.loc[b.lower(),'cap']\n",
    "        if cap_b == np.inf:\n",
    "            return (bal_multiplier, 1)\n",
    "        else:\n",
    "            return (1, 1)\n",
    "    elif b==BAL_TOKEN:\n",
    "        cap_a = whitelist_df.loc[a.lower(),'cap']\n",
    "        if cap_a == np.inf:\n",
    "            return (1, bal_multiplier)\n",
    "        else:\n",
    "            return (1, 1)\n",
    "    else:\n",
    "        return (1, 1)\n",
    "\n",
    "\n",
    "def get_BRF(tokens_weights, bal_multiplier=1):\n",
    "    if type(bal_multiplier) != int: # expect a series of BAL multipliers on the third pass\n",
    "        block_number = tokens_weights.index.get_level_values('block_number').drop_duplicates().values\n",
    "        if len(block_number)>1:\n",
    "            raise Exception('got more than one block_number {}'.format(block_number))\n",
    "        else:\n",
    "            bal_multiplier = final_bal_multiplier[block_number[0]]\n",
    "            \n",
    "    if 'shareholders_subpool' in tokens_weights.index.names:\n",
    "        if tokens_weights.index.get_level_values('shareholders_subpool').all():\n",
    "            bal_multiplier=1\n",
    "    else:\n",
    "        if tokens_weights.index.nlevels==4:\n",
    "            raise Exception('shareholders_subpool not in index')    \n",
    "            \n",
    "    denominator = 0\n",
    "    numerator = 0\n",
    "    token_address = list(tokens_weights.index.get_level_values('token_address'))\n",
    "    token_weights = list(tokens_weights)\n",
    "    for i in range(len(tokens_weights)-1):\n",
    "        for j in range(i+1, len(tokens_weights)):\n",
    "            token_A = token_address[i]\n",
    "            token_B = token_address[j]\n",
    "            weight_A = token_weights[i]\n",
    "            weight_B = token_weights[j]\n",
    "            staking_boosts = get_staking_boosts_of_pair(token_A, token_B, bal_multiplier)\n",
    "            staking_boost_A = staking_boosts[0]\n",
    "            staking_boost_B = staking_boosts[1]\n",
    "            staking_boost_of_pair = (staking_boost_A * weight_A + staking_boost_B * weight_B) / (weight_A + weight_B)\n",
    "            \n",
    "            ratio_factor = 4 * (weight_A / (weight_A + weight_B)) * (weight_B / (weight_A + weight_B))\n",
    "            \n",
    "            numerator += staking_boost_of_pair * ratio_factor * weight_A * weight_B\n",
    "            denominator += weight_A * weight_B\n",
    "    return numerator/denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.facecolor'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "def get_list_of_snapshot_blocks(start, end):\n",
    "    # in the estimator, compute snapshot blocks from start to end\n",
    "    # otherwise estimates and velocity can vary too much if there's a huge change in liquidity mid week\n",
    "    if REALTIME_ESTIMATOR: \n",
    "        block_list = range(start, end, SNAPSHOT_WINDOW_SIZE)\n",
    "    else:\n",
    "        block_list = range(end, start, -SNAPSHOT_WINDOW_SIZE)\n",
    "    block_list = list(block_list)\n",
    "    block_list.sort()\n",
    "    return block_list\n",
    "\n",
    "snapshot_blocks = get_list_of_snapshot_blocks(START_BLOCK, END_BLOCK)\n",
    "snapshot_timestamps_blocks = {w3.eth.getBlock(b).timestamp: b \\\n",
    "                              for b in tqdm(snapshot_blocks, 'Getting snapshot timestamps')}\n",
    "snapshot_blocks_timestamps = {v: k for k,v in snapshot_timestamps_blocks.items()}\n",
    "snapshot_blocks_as_str = [str(b) for b in snapshot_blocks]\n",
    "print('week {}: {} snapshot blocks'.format(WEEK, len(snapshot_blocks)))\n",
    "print('week {}: first snapshot block: {} ({}...)'.format(WEEK, min(snapshot_blocks), snapshot_blocks[:3]))\n",
    "print('week {}: last snapshot block: {} (...{})'.format(WEEK, max(snapshot_blocks), snapshot_blocks[-3:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove kovan\n",
    "whitelist_df.drop(columns=['kovan'], inplace=True)\n",
    "whitelist_df.dropna(inplace=True)\n",
    "# convert string to cap\n",
    "whitelist_df.rename(columns={'homestead':'cap'}, inplace=True)\n",
    "CAP_TIERS = {\n",
    "    'cap1':   1e6,\n",
    "    'cap2':   3e6,\n",
    "    'cap3':  10e6,\n",
    "    'cap4':  30e6,\n",
    "    'cap5': 100e6,\n",
    "    'uncapped': np.inf,\n",
    "}\n",
    "whitelist_df['cap'] = whitelist_df['cap'].apply(lambda x: CAP_TIERS[x])\n",
    "# lower case the token addresses for later join\n",
    "whitelist_df.index.name = 'checksum_token_address'\n",
    "whitelist_df.reset_index(inplace=True)\n",
    "whitelist_df['checksum_token_address'] = whitelist_df['checksum_token_address'].apply(Web3.toChecksumAddress)\n",
    "whitelist_df.set_index(whitelist_df['checksum_token_address'].str.lower(), inplace=True)\n",
    "whitelist_df.index.name = 'token_address'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get decimals of whitelisted tokens\n",
    "import json\n",
    "from web3.exceptions import ABIFunctionNotFound, BadFunctionCallOutput, InvalidAddress\n",
    "token_abi = json.load(open('abi/BToken.json'))\n",
    "def get_token_decimals(token_address):\n",
    "    if token_address == '0x0Ba45A8b5d5575935B8158a88C631E9F9C95a2e5':\n",
    "        return 18\n",
    "    if token_address == '0xE0B7927c4aF23765Cb51314A0E0521A9645F0E2A':\n",
    "        return 9\n",
    "    try:\n",
    "        token_contract = w3.eth.contract(token_address, abi=token_abi)\n",
    "    except InvalidAddress:\n",
    "        print('Invalid Address: ' + token_address)\n",
    "        raise\n",
    "    try:\n",
    "        return token_contract.functions.decimals().call()\n",
    "    except ABIFunctionNotFound:\n",
    "        print(f'{token_address} does not implement decimals(), assuming 0')\n",
    "        return 0\n",
    "    except BadFunctionCallOutput:\n",
    "        print(f'{token_address}: decimals() returned bad output, assuming 0')\n",
    "        return 0\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Getting token decimals from ethereum node...')\n",
    "whitelist_df['decimals'] = whitelist_df['checksum_token_address'].apply(get_token_decimals)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get USD prices of whitelist tokens\n",
    "import requests\n",
    "from time import sleep\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "\n",
    "\n",
    "if REALTIME_ESTIMATOR:\n",
    "    prices_json = {}\n",
    "else:\n",
    "    try:\n",
    "        prices_json = json.load(open(reports_dir+'/_prices.json'))\n",
    "    except FileNotFoundError:\n",
    "        prices_json = {}\n",
    "\n",
    "MARKET_API_URL = 'https://api.coingecko.com/api/v3'\n",
    "price_query = MARKET_API_URL+'/coins/ethereum/contract/{}/market_chart/range?&vs_currency=usd&from={}&to={}'\n",
    "time_bounded_price_query = price_query.format('{}', start_block_timestamp, end_block_timestamp)\n",
    "whitelist_df['prices_api_response'] = ''\n",
    "whitelist_df['prices_dict'] = ''\n",
    "for i in tqdm(whitelist_df.index, 'Getting prices'):\n",
    "    checksum_token_address = whitelist_df.loc[i,'checksum_token_address']\n",
    "    query_url = time_bounded_price_query.format(checksum_token_address)\n",
    "    # when running week 26 we ran across a bug in Coingecko price feed where the price of WETH was reported as 0\n",
    "    # price of ETH is unaffected, so using it instead\n",
    "    if '0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2' in query_url:\n",
    "        query_url = query_url.replace('/contract/0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2','')\n",
    "    prices_dict = {'prices': prices_json.get(checksum_token_address, None)}\n",
    "    if prices_dict['prices'] is None:\n",
    "        prices_dict = None\n",
    "    tries = 0\n",
    "    while prices_dict is None:\n",
    "        token_prices = requests.get(query_url)\n",
    "        try:\n",
    "            prices_dict = json.loads(token_prices.content)\n",
    "        except:\n",
    "            pass\n",
    "        # sleep for one second to avoid being blocked\n",
    "        sleep(1)\n",
    "        tries += 1\n",
    "        if tries > 5:\n",
    "            break\n",
    "        whitelist_df.loc[i,'prices_api_response'] = token_prices.content\n",
    "    whitelist_df.loc[i,'prices_dict'] = [prices_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "tokens_not_found = whitelist_df[whitelist_df['prices_dict'].apply(lambda x: 'error' in x.keys())].index\n",
    "whitelist_df.drop(index=tokens_not_found, inplace=True)\n",
    "print('Tokens not found in Coingecko: {}'.format(list(tokens_not_found)))\n",
    "\n",
    "whitelist_df['prices_lists'] = whitelist_df['prices_dict'].apply(lambda x: x.get('prices'))\n",
    "prices_not_found = whitelist_df[whitelist_df['prices_lists'].apply(lambda x: len(x)==0)].index\n",
    "whitelist_df.drop(index=prices_not_found, inplace=True)\n",
    "print('Prices not found in Coingecko for: {}'.format(list(prices_not_found)))\n",
    "\n",
    "exploded_whitelist_df = whitelist_df.explode('prices_lists').dropna()\n",
    "exploded_whitelist_df.reset_index(inplace=True)\n",
    "exploded_whitelist_df[['timestamp','price']] = pd.DataFrame(exploded_whitelist_df.prices_lists.tolist(), index=exploded_whitelist_df.index)\n",
    "\n",
    "prices_df = exploded_whitelist_df[['token_address', 'checksum_token_address', 'cap', 'timestamp', 'price']].copy()\n",
    "\n",
    "prices_df['ts_price'] = prices_df.apply(lambda x: [x['timestamp'], x['price']], axis=1)\n",
    "\n",
    "if not REALTIME_ESTIMATOR:\n",
    "    prices_df.groupby('checksum_token_address').agg(list)['ts_price'].to_json(reports_dir+'/_prices.json',\n",
    "                                                                 orient='index',\n",
    "                                                                 indent=4\n",
    "                                                                )\n",
    "\n",
    "prices_df['timestamp'] = prices_df['timestamp']//1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get eligible token balances of every balancer pool at every snapshot block from Big Query\n",
    "get_pools_sql = '''\n",
    "SELECT pool FROM `blockchain-etl.ethereum_balancer.BFactory_event_LOG_NEW_POOL` \n",
    "'''\n",
    "\n",
    "sql = \"\"\"\n",
    "select * from `blockchain-etl.ethereum_balancer.view_token_balances_subset`\n",
    "where token_address in (\\'{0}\\')\n",
    "and address in ({1})\n",
    "and token_address not in ('0xd46ba6d942050d489dbd938a2c909a5d5039a161')\n",
    "and block_number in ({2})\n",
    "and balance > 0\n",
    "\n",
    "union all\n",
    "\n",
    "select '0xd46ba6d942050d489dbd938a2c909a5d5039a161' as token_address,\n",
    "* from `blockchain-etl.ethereum_balancer.view_token_balances_subset_AMPL`\n",
    "where address in ({1})\n",
    "and block_number in ({2})\n",
    "and balance > 0\n",
    "\n",
    "\"\"\".format('\\',\\''.join(whitelist_df.index), # only get balances of tokens for which there is a price feed\n",
    "           get_pools_sql, \n",
    "           ','.join(snapshot_blocks_as_str))\n",
    "# print(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud import bigquery_storage\n",
    "import time\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Querying BigQuery...')\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "bqstorageclient = bigquery_storage.BigQueryReadClient()\n",
    "pools_balances = (\n",
    "    client.query(sql)\n",
    "    .result()\n",
    "    .to_dataframe(bqstorage_client=bqstorageclient)\n",
    ")\n",
    "n = len(pools_balances)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + f' - Done ({n} records)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pools_balances['scaled_balance'] = pools_balances['balance'] * pools_balances.join(whitelist_df['decimals'], on='token_address')['decimals'].apply(lambda x: 10**(-x))\n",
    "pools_balances['timestamp'] = pools_balances['block_number'].apply(lambda x: snapshot_blocks_timestamps[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pools_balances.set_index(['address','block_number','token_address'], inplace=True)\n",
    "number_of_liquid_eligible_tokens = pools_balances.groupby(['address','block_number']).size()\n",
    "number_of_liquid_eligible_tokens.name = 'number_of_liquid_eligible_tokens'\n",
    "pools_balances = pools_balances.join(number_of_liquid_eligible_tokens)\n",
    "eligible_pools_balances = pools_balances[pools_balances['number_of_liquid_eligible_tokens']>=2].copy()\n",
    "eligible_pools_balances.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge balances and prices datasets on nearest timestamp, and compute USD balance of each token in each pool at each block\n",
    "usd_pools_balances = pd.merge_asof(eligible_pools_balances.sort_values(by='timestamp'), \n",
    "                                   prices_df.sort_values(by='timestamp'), \n",
    "                                   on='timestamp', by='token_address', direction='nearest')\n",
    "usd_pools_balances['usd_balance'] = usd_pools_balances['scaled_balance'] * usd_pools_balances['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get token weights and swap fees of pools with public swap enabled\n",
    "sql = \"\"\"\n",
    "select W.*, COALESCE(swapFee,'1000000000000') as swapfee\n",
    "from `blockchain-etl.ethereum_balancer.view_pools_tokens_denorm_weights` W\n",
    "left join `blockchain-etl.ethereum_balancer.view_pools_fees` F\n",
    "on W.address = F.address and W.block_number = F.block_number\n",
    "left join `blockchain-etl.ethereum_balancer.view_pools_public_swaps` S\n",
    "on W.address = S.address and W.block_number = S.block_number\n",
    "where COALESCE(public_,'false') = 'true'\n",
    "and W.denorm > 0\n",
    "and token_address in (\\'{}\\')\n",
    "and W.address in ('{}')\n",
    "and W.block_number in ({})\n",
    "\"\"\".format('\\',\\''.join(whitelist_df.index), # only get weights of tokens for which there is a price feed\n",
    "           '\\',\\''.join(eligible_pools_balances['address'].drop_duplicates()), \n",
    "           ','.join(snapshot_blocks_as_str))\n",
    "# print(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud import bigquery_storage\n",
    "import time\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Querying BigQuery...')\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "bqstorageclient = bigquery_storage.BigQueryReadClient()\n",
    "pools_weights = (\n",
    "    client.query(sql)\n",
    "    .result()\n",
    "    .to_dataframe(bqstorage_client=bqstorageclient)\n",
    ")\n",
    "n = len(pools_weights)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + f' - Done ({n} records)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the merge removes records associated with balances of tokens that are not part of the pool\n",
    "pools_weights_balances = pools_weights.merge(usd_pools_balances, \n",
    "                                             on=['address', 'token_address', 'block_number'],\n",
    "                                             how='inner')\n",
    "pools_weights_balances.set_index(['address', 'token_address', 'block_number'], inplace=True)\n",
    "pools_weights_balances['denorm'] = pools_weights_balances['denorm'].apply(float)\n",
    "summed_weights = pools_weights_balances['denorm'].groupby(['address','block_number']).sum()\n",
    "norm_weights = pools_weights_balances['denorm'] / summed_weights\n",
    "norm_weights.name = 'norm_weights'\n",
    "pools_weights_balances = pools_weights_balances.join(norm_weights)\n",
    "# remove pools with only one eligible token\n",
    "pools_weights_balances = pools_weights_balances[pools_weights_balances['norm_weights']<1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Computing wrap factor...')\n",
    "wrap_factor = pools_weights_balances['norm_weights'].\\\n",
    "                    groupby(['address','block_number']).\\\n",
    "                    agg(get_wrap_factor)\n",
    "wrap_factor.name = 'wrap_factor'\n",
    "pools_weights_balances = pools_weights_balances.join(wrap_factor)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Computing BRF (first pass)...')\n",
    "brf = pools_weights_balances['norm_weights']. \\\n",
    "                groupby(['address','block_number']). \\\n",
    "                agg(get_BRF)\n",
    "brf.name = 'first_pass_brf'\n",
    "pools_weights_balances = pools_weights_balances.join(brf)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the fee factor\n",
    "# https://forum.balancer.finance/t/modifying-feefactor-toward-reducing-the-mining-penalty-for-high-fee-pools/103\n",
    "# a swapfee of 1% is stored in the smart contracts as 1e+16 (0.01e+18)\n",
    "# fee factor formula as defined in the specs above takes as argument the fee as a percentage (eg 1 for a fee of 1% - not 0.01)\n",
    "pools_weights_balances['fee_factor'] = np.exp(-(0.25 * \\\n",
    "                                                (100 * \\\n",
    "                                                 (pools_weights_balances['swapfee'].astype(float) / 1E18)))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pools_weights_balances['adjustedLiquidityPreTokenCap'] = pools_weights_balances['usd_balance'] * \\\n",
    "                                                            pools_weights_balances['fee_factor'] * \\\n",
    "                                                            pools_weights_balances['wrap_factor'] * \\\n",
    "                                                            pools_weights_balances['first_pass_brf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the tokenCapFactor for each token_address at each block_number\n",
    "tokenCapFactor = np.minimum(1, whitelist_df['cap'] / (pools_weights_balances['adjustedLiquidityPreTokenCap'].\\\n",
    "    groupby(['block_number','token_address']).\\\n",
    "    sum().\\\n",
    "    sort_values()))\n",
    "tokenCapFactor.name = 'tokenCapFactor'\n",
    "pools_weights_balances = pools_weights_balances.join(tokenCapFactor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pools_weights_balances['token_capped_usd_balance'] = pools_weights_balances['usd_balance'] * \\\n",
    "                                                        pools_weights_balances['tokenCapFactor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get liquidity providers and the amount of BPT each has\n",
    "# private and smart pools don't have BPT, so we assign 1 fictitious BPT to the controller\n",
    "# BAL mined by controllers of smart pools created by the CRPFactory will be redistributed to the controller token holders later in the process\n",
    "sql = \"\"\"\n",
    "with shared_pools as (\n",
    "  select token_address as address, address as bpt_holder, block_number, balance as bpt from `blockchain-etl.ethereum_balancer.view_token_balances_subset`\n",
    "  where token_address in ('{0}')\n",
    "  and block_number in ({1})\n",
    "  and balance > 0\n",
    "),\n",
    "private_pools as (\n",
    "  select address, controller as bpt_holder, block_number, 1 as bpt from `blockchain-etl.ethereum_balancer.view_pools_controllers`\n",
    "  where address not in (select address from shared_pools)\n",
    "  and block_number in ({1})\n",
    ")\n",
    "select * from shared_pools\n",
    "union all\n",
    "select * from private_pools\n",
    "\"\"\".format('\\',\\''.join(pools_weights_balances.index.get_level_values('address').drop_duplicates()), \n",
    "           ','.join(snapshot_blocks_as_str))\n",
    "# print(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud import bigquery_storage\n",
    "import time\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Querying BigQuery...')\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "bqstorageclient = bigquery_storage.BigQueryReadClient()\n",
    "bpt_balances = (\n",
    "    client.query(sql)\n",
    "    .result()\n",
    "    .to_dataframe(bqstorage_client=bqstorageclient)\n",
    ")\n",
    "n = len(bpt_balances)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + f' - Done ({n} records)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_shareholder = bpt_balances['bpt_holder'].isin(BLACKLISTED_SHAREHOLDERS_lower)\n",
    "bpt_balances['is_shareholder'] = is_shareholder\n",
    "bpt_balances.set_index(['address','block_number','is_shareholder','bpt_holder'], inplace=True)\n",
    "bpt_balances.rename_axis(index={'is_shareholder': 'shareholders_subpool'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split pools that have a blacklisted shareholder as one of their LPs\n",
    "split_pools = bpt_balances['bpt'].groupby(['address','block_number','shareholders_subpool']).sum()\n",
    "total_bpt = bpt_balances['bpt'].groupby(['address','block_number']).sum()\n",
    "relative_size_of_subpool = split_pools/total_bpt\n",
    "relative_size_of_subpool.name = 'relative_size_of_subpool'\n",
    "subpools = pools_weights_balances.join(relative_size_of_subpool, how='inner')\n",
    "\n",
    "# recompute values according to the relative size of the subpool\n",
    "splitable_cols = ['balance', 'scaled_balance', 'usd_balance', 'adjustedLiquidityPreTokenCap', \n",
    "                'token_capped_usd_balance']\n",
    "for c in splitable_cols:\n",
    "    subpools[c] = subpools[c] * subpools['relative_size_of_subpool']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMP_BAL_MULTIPLIER = 3\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Second BRF - no BAL multiplier...')\n",
    "\n",
    "brf = subpools['norm_weights']. \\\n",
    "                groupby(['address','block_number','shareholders_subpool']). \\\n",
    "                agg(get_BRF)\n",
    "brf.name = 'second_pass_brf_mult1'\n",
    "subpools = subpools.join(brf)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Done')\n",
    "\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + f' - Second BRF - with temp BAL multiplier ({TEMP_BAL_MULTIPLIER})...')\n",
    "brf = subpools['norm_weights']. \\\n",
    "                groupby(['address','block_number','shareholders_subpool']). \\\n",
    "                agg(get_BRF, bal_multiplier = TEMP_BAL_MULTIPLIER)\n",
    "brf.name = 'second_pass_brf_with_temp_mult'\n",
    "subpools = subpools.join(brf)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subpools['adjustedLiquidityPreStaking'] = subpools['token_capped_usd_balance'] * \\\n",
    "                                            subpools['fee_factor'] * \\\n",
    "                                            subpools['wrap_factor'] * \\\n",
    "                                            subpools['second_pass_brf_mult1']\n",
    "\n",
    "subpools['adjustedLiquidityWithTempStakingMult'] = subpools['token_capped_usd_balance'] * \\\n",
    "                                            subpools['fee_factor'] * \\\n",
    "                                            subpools['wrap_factor'] * \\\n",
    "                                            subpools['second_pass_brf_with_temp_mult']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute final BAL multiplier\n",
    "total_adjustedLiquidityPreStaking = subpools['adjustedLiquidityPreStaking'].groupby('block_number').sum()\n",
    "total_adjustedLiquidityWithStakingTempMult = subpools['adjustedLiquidityWithTempStakingMult'].groupby('block_number').sum()\n",
    "final_desired_adjusted_liquidity = total_adjustedLiquidityPreStaking / (1-STAKERS_SHARE)\n",
    "stretch = (final_desired_adjusted_liquidity - total_adjustedLiquidityPreStaking) / \\\n",
    "            (total_adjustedLiquidityWithStakingTempMult - total_adjustedLiquidityPreStaking)\n",
    "final_bal_multiplier = 1 + stretch * (TEMP_BAL_MULTIPLIER - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Third BRF - with final BAL multiplier...')\n",
    "brf = subpools['norm_weights']. \\\n",
    "                groupby(['address','block_number','shareholders_subpool']). \\\n",
    "                agg(get_BRF, bal_multiplier = final_bal_multiplier)\n",
    "brf.name = 'third_pass_brf_with_final_mult'\n",
    "subpools = subpools.join(brf)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the final adjusted liquidity of each token in each subpool at each block\n",
    "subpools['finalAdjustedLiquidity'] = subpools['token_capped_usd_balance'] * \\\n",
    "                                            subpools['fee_factor'] * \\\n",
    "                                            subpools['wrap_factor'] * \\\n",
    "                                            subpools['third_pass_brf_with_final_mult']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the total final adjusted liquidity at each block\n",
    "total_final_adjustedLiquidity = subpools['finalAdjustedLiquidity'].groupby('block_number').sum()\n",
    "\n",
    "# compute the share of liquidity provided by each token in each subpool\n",
    "share_of_liquidity = subpools['finalAdjustedLiquidity'] / total_final_adjustedLiquidity\n",
    "share_of_liquidity.name = 'share_of_liquidity'\n",
    "subpools = subpools.join(share_of_liquidity)\n",
    "\n",
    "# compute the BAL mined by each token in each subpool at each block, proportional to the share of liquidity\n",
    "subpools['BAL_mined'] = subpools['share_of_liquidity'] * WEEKLY_MINED / len(snapshot_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the BAL mined by each LP proportional to their share of the pool\n",
    "bal_mined_by_subpool_per_block = subpools['BAL_mined']. \\\n",
    "                                    groupby(['address','block_number','shareholders_subpool']). \\\n",
    "                                    sum()\n",
    "\n",
    "total_bpt = bpt_balances['bpt'].groupby(['address','block_number','shareholders_subpool']).sum()\n",
    "share_of_pool = bpt_balances['bpt'] / total_bpt\n",
    "\n",
    "bal_mined = bpt_balances.copy()\n",
    "bal_mined['bal_mined'] = (bal_mined_by_subpool_per_block * share_of_pool)\n",
    "bal_mined.reset_index(inplace=True)\n",
    "chksums = {x: Web3.toChecksumAddress(x) for x in bal_mined['bpt_holder'].drop_duplicates()}\n",
    "bal_mined['chksum_bpt_holder'] = bal_mined['bpt_holder'].apply(lambda x: chksums[x])\n",
    "bal_mined.set_index(['address', 'block_number', 'shareholders_subpool', 'chksum_bpt_holder'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals = bal_mined['bal_mined'].groupby('chksum_bpt_holder').sum()\n",
    "\n",
    "if not REALTIME_ESTIMATOR:\n",
    "    totals[totals>=CLAIM_THRESHOLD].apply(lambda x: format(x, f'.{CLAIM_PRECISION}f')).to_json(reports_dir+'/_totalsPreRedirect.json',\n",
    "                                                  indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not REALTIME_ESTIMATOR:\n",
    "    # save pools time series\n",
    "    mined_by_pools = subpools['BAL_mined'].groupby(['address','block_number']).sum()\n",
    "    mined_by_pools = mined_by_pools[mined_by_pools>=CLAIM_THRESHOLD].apply(lambda x: format(x, f'.{CLAIM_PRECISION}f'))\n",
    "    mined_by_pools = pd.DataFrame(mined_by_pools).reset_index()\n",
    "    mined_by_pools = mined_by_pools.pivot(index='address', columns='block_number', values='BAL_mined')\n",
    "    mined_by_pools.to_json(reports_dir+'/_poolsSeries.json.zip',\n",
    "                           orient='index',\n",
    "                           indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redirect and Redistribute\n",
    "Recursively:\n",
    "* redirects BAL earned by one address to another\n",
    "* redistributes BAL earned by a smart contract to its token holders\n",
    "  * a smart contract can earn BAL if it is the controller of a private pool (eg smart pools) or if it holds BPT of finalized pools (eg staking contracts)\n",
    "  * by doing this recursively we also account for staking contracts that hold BPTs of smart pools (BAL earned by the CRP is redistributed to its token holders; then the subset of BAL that goes to the staking contract is redistributed to its holders)\n",
    "  * all CRPs created via the CRPFactory are redistributers by default. Other contracts can PR into `config/redistribute.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get addresses that redirect\n",
    "if REALTIME_ESTIMATOR:\n",
    "    url = 'https://raw.githubusercontent.com/balancer-labs/bal-mining-scripts/master/config/redirect.json'\n",
    "    jsonurl = urlopen(url)\n",
    "    redirects = json.loads(jsonurl.read())\n",
    "else:\n",
    "    redirects = json.load(open('config/redirect.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get addresses that redistribute\n",
    "if REALTIME_ESTIMATOR:\n",
    "    url = 'https://raw.githubusercontent.com/balancer-labs/bal-mining-scripts/master/config/redistribute.json'\n",
    "    jsonurl = urlopen(url)\n",
    "    redistributers_dict = json.loads(jsonurl.read())\n",
    "else:\n",
    "    redistributers_dict = json.load(open('config/redistribute.json'))\n",
    "redistributers_list = list(redistributers_dict.keys())\n",
    "# get list of CRPs\n",
    "sql = 'SELECT pool FROM `blockchain-etl.ethereum_balancer.CRPFactory_event_LogNewCrp`'\n",
    "# Requires setting the environment variable GOOGLE_APPLICATION_CREDENTIALS \n",
    "# to the file path of the JSON file that contains a service account key \n",
    "# with access to the token_balances_subset view\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "bqstorageclient = bigquery_storage.BigQueryReadClient()\n",
    "crps = (\n",
    "    client.query(sql)\n",
    "    .result()\n",
    "    .to_dataframe(bqstorage_client=bqstorageclient)\n",
    ")\n",
    "\n",
    "redistributers_list.extend(crps['pool'].drop_duplicates().apply(Web3.toChecksumAddress))\n",
    "# print('Redistributers: {}'.format(redistributers_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get redistributers' token holders\n",
    "sql = \"\"\"\n",
    "select * from `blockchain-etl.ethereum_balancer.view_token_balances_subset`\n",
    "where token_address in ({})\n",
    "and block_number in ({})\n",
    "and balance > 0\n",
    "\"\"\".format('\\''+'\\',\\''.join(map(lambda x: x.lower(), redistributers_list))+'\\'', \n",
    "           ','.join(snapshot_blocks_as_str))\n",
    "# print(sql)\n",
    "\n",
    "# Requires setting the environment variable GOOGLE_APPLICATION_CREDENTIALS \n",
    "# to the file path of the JSON file that contains a service account key \n",
    "\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Querying Bigquery for the token holders of the redistributers ...')\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "bqstorageclient = bigquery_storage.BigQueryReadClient()\n",
    "running_balances = (\n",
    "    client.query(sql)\n",
    "    .result()\n",
    "    .to_dataframe(bqstorage_client=bqstorageclient)\n",
    ")\n",
    "running_balances['balance'] = running_balances['balance'].astype(float)\n",
    "running_balances['address'] = running_balances['address'].apply(Web3.toChecksumAddress)\n",
    "running_balances['token_address'] = running_balances['token_address'].apply(Web3.toChecksumAddress)\n",
    "running_balances = running_balances.rename(columns={\"token_address\": \"redistributer\", \"address\": \"share_holder\"})\n",
    "running_balances.set_index(['block_number','redistributer','share_holder'], inplace=True)\n",
    "\n",
    "shares = pd.DataFrame(running_balances['balance'])/pd.DataFrame(running_balances.groupby(['block_number','redistributer']).sum()['balance'])\n",
    "shares.columns = ['perc_share']\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miners = bal_mined['bal_mined'].groupby(['block_number', 'chksum_bpt_holder']).sum().reset_index()\n",
    "miners = miners[miners['bal_mined']>0]\n",
    "miners.rename(columns={'chksum_bpt_holder':'miner'}, inplace=True)\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    n = len(miners['miner'].drop_duplicates()[miners['miner'].drop_duplicates().isin(redirects.keys())])\n",
    "    print(f'Redirect #{i+1}: {n} redirectors found')\n",
    "    # redirect\n",
    "    miners['miner'] = miners['miner'].apply(lambda x: redirects.get(x,x))\n",
    "\n",
    "    n = len(miners['miner'].drop_duplicates()[miners['miner'].drop_duplicates().isin(redistributers_list)])\n",
    "    print(f'Redistribute #{i+1}: {n} Redistributors found')\n",
    "    # redistribute\n",
    "    # first assume all miners are redistributing\n",
    "    miners.rename(columns={'miner':'redistributer'}, inplace=True)\n",
    "    miners.set_index(['block_number', 'redistributer'], inplace=True)\n",
    "    # join with shares of redistributing contracts\n",
    "    miners = miners.join(shares, how='left').reset_index()\n",
    "    # miners are the shareholders of the redistributing contracts; or the original redistributer if NA\n",
    "    miners['miner'] = miners['share_holder'].fillna(miners['redistributer'])\n",
    "    # the share of BAL for each miner is the share of the redistribution contract they own; or 1 if NA\n",
    "    miners['perc_share'] = miners['perc_share'].fillna(1)\n",
    "    # compute BAL earned by each miner\n",
    "    miners['bal_mined'] = miners['bal_mined'] * miners['perc_share']\n",
    "    # at this point, same miner might earn BAL from different sources, so we need to aggregate again\n",
    "    miners = miners[['block_number', 'miner', 'bal_mined']].groupby(['block_number', 'miner']).sum().reset_index()\n",
    "    \n",
    "    redirecters_remain = miners['miner'].drop_duplicates().isin(redirects.keys()).any()\n",
    "    redistributers_remain = miners['miner'].drop_duplicates().isin(redistributers_list).any()\n",
    "    if not redirecters_remain and not redistributers_remain:\n",
    "        break\n",
    "\n",
    "totals = miners[['miner','bal_mined']].groupby('miner').sum()['bal_mined']\n",
    "\n",
    "if not REALTIME_ESTIMATOR:\n",
    "    if gov_factor > 1:\n",
    "        filename = '/_totalsPreGovFactor.json'\n",
    "    else:\n",
    "        filename = '/_totalsLiquidityMining.json'\n",
    "\n",
    "    totals[totals>=CLAIM_THRESHOLD].apply(lambda x: \\\n",
    "                                          format(x, f'.{CLAIM_PRECISION}f')).\\\n",
    "                                            to_json(reports_dir+filename,\n",
    "                                                    indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gov Factor\n",
    "Liquidity providers that participate in the governance of Balancer get a bonus on the BAL earned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply govFactor\n",
    "if gov_factor > 1:\n",
    "    voters = []\n",
    "    for i,p in latest_gov_proposals.iterrows():\n",
    "        voters_url = f'https://hub.snapshot.page/api/balancer/proposal/{i}'\n",
    "        prop_voters = list(json.loads(requests.get(voters_url).content).keys())\n",
    "        print(f'{len(prop_voters)} addresses voted on proposal {i}')\n",
    "        voters.extend(prop_voters)\n",
    "\n",
    "        # get delegators and delegatees\n",
    "        url = 'https://api.thegraph.com/subgraphs/name/snapshot-labs/snapshot'\n",
    "        query = f'''query {{\n",
    "          delegations(block: {{number: {p['msg']['payload'].get('snapshot',0)}}}, \n",
    "              first: 1000, \n",
    "              where: {{space_in: [\"balancer\", \"\"]}}) {{\n",
    "                delegate\n",
    "                delegator\n",
    "          }}\n",
    "        }}'''\n",
    "        r = requests.post(url, json = {'query':query})\n",
    "        delegations = pd.DataFrame(json.loads(r.content)['data']['delegations'])\n",
    "        if len(delegations)==1000:\n",
    "                warnings.warn('Delegations reached 1000, implement pagination')\n",
    "        delegators_that_voted_indirectly = delegations[delegations.delegate.isin(map(lambda x: x.lower(), voters))]['delegator']\n",
    "        print(f'{len(delegators_that_voted_indirectly)} addresses voted through delegators')\n",
    "        voters.extend(map(Web3.toChecksumAddress, delegators_that_voted_indirectly))\n",
    "    \n",
    "    voters = list(dict.fromkeys(voters)) #drop duplicates\n",
    "    print(f'{len(voters)} total unique voters')\n",
    "    \n",
    "    totals_pre_govfactor = totals.copy()\n",
    "    totals[totals.index.isin(voters)] = totals * gov_factor\n",
    "    \n",
    "    expanded_BAL_mined = totals.sum()\n",
    "    \n",
    "    totals = totals * WEEKLY_MINED / expanded_BAL_mined\n",
    "    totals_post_govfactor = totals.copy()\n",
    "\n",
    "    print('govFactor expansion: {:.2f}'.format(expanded_BAL_mined/WEEKLY_MINED))\n",
    "    print('BAL post-govFactor: {:.18f}'.format(totals.sum()))\n",
    "    \n",
    "    if not REALTIME_ESTIMATOR:\n",
    "        filename = '/_totalsLiquidityMining.json'\n",
    "        totals[totals>=CLAIM_THRESHOLD].apply(lambda x: \\\n",
    "                                              format(x, f'.{CLAIM_PRECISION}f')).\\\n",
    "                                                to_json(reports_dir+filename,\n",
    "                                                        indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update real time estimates in GBQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REALTIME_ESTIMATOR:\n",
    "    # zero previous week's velocity\n",
    "    sql = f'''\n",
    "        UPDATE {project_id}.bal_mining_estimates.pool_estimates\n",
    "        SET velocity = '0'\n",
    "        WHERE week = {WEEK-1}\n",
    "    '''\n",
    "    client = bigquery.Client()\n",
    "    query = client.query(sql)\n",
    "    query.result();\n",
    "    sql = f'''\n",
    "        UPDATE {project_id}.bal_mining_estimates.lp_estimates\n",
    "        SET velocity = '0'\n",
    "        WHERE week = {WEEK-1}\n",
    "    '''\n",
    "    client = bigquery.Client()\n",
    "    query = client.query(sql)\n",
    "    query.result();\n",
    "\n",
    "    # write to GBQ (LPs)\n",
    "    cur_estimate = pd.DataFrame(totals)\n",
    "    cur_estimate.columns = ['earned']\n",
    "    cur_estimate.index.name = 'address'\n",
    "    \n",
    "    try:\n",
    "        prev_estimate = pd.read_gbq(f'select address, earned, timestamp from bal_mining_estimates.lp_estimates WHERE week = {WEEK}', \n",
    "                        project_id=os.environ['GCP_PROJECT'])\n",
    "        prev_estimate.set_index('address', inplace=True)\n",
    "        prev_estimate_timestamp = prev_estimate.iloc[0]['timestamp']\n",
    "    except:\n",
    "        prev_estimate_timestamp = 0\n",
    "    if prev_estimate_timestamp < start_block_timestamp:\n",
    "        #previous estimate is last week's; compute velocity from end_block_timestamp and start_block_timestamp\n",
    "        delta_t = (end_block_timestamp - start_block_timestamp)\n",
    "        earned = cur_estimate['earned'].astype(float)\n",
    "        cur_estimate['velocity'] = (earned/delta_t).apply(lambda x: format(x, f'.{18}f'))\n",
    "    else:\n",
    "        #compute velocity based on increase and time passed\n",
    "        delta_t = (end_block_timestamp - prev_estimate_timestamp)\n",
    "        diff_estimate = cur_estimate.join(prev_estimate, rsuffix='_prev').fillna(0)\n",
    "        cur_earned = diff_estimate['earned'].astype(float)\n",
    "        prev_earned = diff_estimate['earned_prev'].astype(float)\n",
    "        cur_estimate['velocity'] = ((cur_earned-prev_earned)/delta_t).apply(lambda x: format(x, f'.{18}f'))\n",
    "        \n",
    "    # delete this week's previous estimates\n",
    "    sql = f'''\n",
    "        DELETE FROM {project_id}.bal_mining_estimates.lp_estimates\n",
    "        WHERE week = {WEEK}\n",
    "    '''\n",
    "    client = bigquery.Client()\n",
    "    query = client.query(sql)\n",
    "    query.result();\n",
    "\n",
    "    cur_estimate['earned'] = cur_estimate['earned'].apply(lambda x: format(x, f'.{18}f'))\n",
    "    cur_estimate['timestamp'] = end_block_timestamp\n",
    "    cur_estimate['week'] = WEEK\n",
    "    cur_estimate.reset_index(inplace=True)\n",
    "    cur_estimate.to_gbq( 'bal_mining_estimates.lp_estimates', \n",
    "                        project_id=os.environ['GCP_PROJECT'], \n",
    "                        if_exists='append')\n",
    "\n",
    "\n",
    "    \n",
    "    # write to GBQ (pools)\n",
    "    cur_estimate = pd.DataFrame(subpools['BAL_mined'].groupby('address').sum())\n",
    "    cur_estimate.columns = ['earned']\n",
    "    cur_estimate['earned'] = cur_estimate['earned'].apply(lambda x: format(x, f'.{18}f'))\n",
    "    cur_estimate.index.name = 'address'\n",
    "    \n",
    "    try:\n",
    "        prev_estimate = pd.read_gbq(f'select address, earned, timestamp from bal_mining_estimates.pool_estimates WHERE week = {WEEK}', \n",
    "                        project_id=os.environ['GCP_PROJECT'])\n",
    "        prev_estimate.set_index('address', inplace=True)\n",
    "        prev_estimate_timestamp = prev_estimate.iloc[0]['timestamp']\n",
    "    except:\n",
    "        prev_estimate_timestamp = 0\n",
    "    if prev_estimate_timestamp < start_block_timestamp:\n",
    "        #previous estimate is last week's; compute velocity from end_block_timestamp and start_block_timestamp\n",
    "        delta_t = (end_block_timestamp - start_block_timestamp)\n",
    "        earned = cur_estimate['earned'].astype(float)\n",
    "        cur_estimate['velocity'] = (earned/delta_t).apply(lambda x: format(x, f'.{18}f'))\n",
    "    else:\n",
    "        #compute velocity based on increase and time passed\n",
    "        delta_t = (end_block_timestamp - prev_estimate_timestamp)\n",
    "        diff_estimate = cur_estimate.join(prev_estimate, rsuffix='_prev').fillna(0)\n",
    "        cur_earned = diff_estimate['earned'].astype(float)\n",
    "        prev_earned = diff_estimate['earned_prev'].astype(float)\n",
    "        cur_estimate['velocity'] = ((cur_earned-prev_earned)/delta_t).apply(lambda x: format(x, f'.{18}f'))\n",
    "        \n",
    "    # delete this week's previous estimates\n",
    "    project_id = os.environ['GCP_PROJECT']\n",
    "    sql = f'''\n",
    "        DELETE FROM {project_id}.bal_mining_estimates.pool_estimates\n",
    "        WHERE week = {WEEK}\n",
    "    '''\n",
    "    client = bigquery.Client()\n",
    "    query = client.query(sql)\n",
    "    query.result();\n",
    "\n",
    "    cur_estimate['timestamp'] = end_block_timestamp\n",
    "    cur_estimate['week'] = WEEK\n",
    "    cur_estimate.reset_index(inplace=True)\n",
    "    cur_estimate.to_gbq( 'bal_mining_estimates.pool_estimates', \n",
    "                        project_id=os.environ['GCP_PROJECT'], \n",
    "                        if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gas Reimbursement Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.bal4gas import compute_bal_for_gas\n",
    "\n",
    "if not REALTIME_ESTIMATOR:\n",
    "    whitelist = pd.read_json(f'https://raw.githubusercontent.com/balancer-labs/assets/w{WEEK}/lists/eligible.json').index.values\n",
    "    gas_whitelist = pd.Series(whitelist).str.lower().tolist()\n",
    "    gas_whitelist.append('0xeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee')\n",
    "#     gas_whitelist = ['0xc02aaa39b223fe8d0a0e5c4f27ead9083c756cc2',\n",
    "#                      '0xa0b86991c6218b36c1d19d4a2e9eb0ce3606eb48',\n",
    "#                      '0x6b175474e89094c44da98b954eedeac495271d0f',\n",
    "#                      '0xba100000625a3754423978a60c9317c58a424e3d',\n",
    "#                      '0x2260fac5e5542a773aa44fbcfedf7c193bc2c599']\n",
    "\n",
    "    \n",
    "    merge = compute_bal_for_gas(start_block_timestamp, end_block_timestamp, gas_whitelist, plot=True, verbose=True)\n",
    "\n",
    "    totals_bal4gas = merge[['address','bal_reimbursement']].groupby('address').sum()['bal_reimbursement']\n",
    "    totals_bal4gas[totals_bal4gas>=CLAIM_THRESHOLD].apply(\\\n",
    "       lambda x: format(x, f'.{CLAIM_PRECISION}f')).to_json(reports_dir+'/_gasReimbursement.json',\n",
    "       indent=4)\n",
    "    print(f'BAL reimbursements for the week: {sum(totals_bal4gas)}')\n",
    "\n",
    "    # combine BAL from liquidity mining and gas reimbursements\n",
    "    totals = pd.DataFrame(totals).join(totals_bal4gas, how='outer')\n",
    "    totals.fillna(0, inplace=True)\n",
    "    totals['bal_total'] = totals['bal_mined'] + totals['bal_reimbursement']\n",
    "    totals = totals['bal_total']\n",
    "    totals[totals>=CLAIM_THRESHOLD].apply(\\\n",
    "       lambda x: format(x, f'.{CLAIM_PRECISION}f')).to_json(reports_dir+'/_totals.json',\n",
    "       indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tokens = subpools['BAL_mined'].groupby(['token_address']).sum().sort_values(ascending=False).head(10).index\n",
    "subpools['datetime'] = pd.to_datetime(subpools.timestamp, unit='s')\n",
    "rewards_per_token = subpools.groupby(['token_address','datetime']).sum()['BAL_mined']\n",
    "print('Top 10 tokens:\\n' + '\\n'.join(top_tokens))\n",
    "if not REALTIME_ESTIMATOR:\n",
    "    ax = pd.DataFrame(rewards_per_token).reset_index().\\\n",
    "        pivot(index='datetime', columns='token_address', values='BAL_mined')[top_tokens].\\\n",
    "        plot(figsize = (15,10),\n",
    "             title = 'BAL mined by top 10 tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_per_pool = subpools.groupby(['address','datetime']).sum()['BAL_mined']\n",
    "top_pools = subpools['BAL_mined'].groupby(['address']).sum().sort_values(ascending=False).head(10).index\n",
    "print('Top 10 pools:\\n' + '\\n'.join(top_pools))\n",
    "if not REALTIME_ESTIMATOR:\n",
    "    ax = pd.DataFrame(rewards_per_pool).reset_index().\\\n",
    "        pivot(index='datetime', columns='address', values='BAL_mined')[top_pools].\\\n",
    "        plot(figsize = (15,10),\n",
    "             title = 'BAL earned by top 10 pools')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_per_lp = bal_mined['bal_mined'].groupby(['chksum_bpt_holder','block_number']).sum()\n",
    "top_lps = bal_mined['bal_mined'].groupby(['chksum_bpt_holder']).sum().sort_values(ascending=False).head(10).index\n",
    "print('Top 10 LPs:\\n' + '\\n'.join(top_lps))\n",
    "if not REALTIME_ESTIMATOR:\n",
    "    ax = pd.DataFrame(rewards_per_lp).reset_index().\\\n",
    "        pivot(index='block_number', columns='chksum_bpt_holder', values='bal_mined')[top_lps].\\\n",
    "        plot(figsize = (15,10),\n",
    "             title = 'BAL earned by top 10 liquidity providers')\n",
    "    ax.ticklabel_format(axis='x', style='plain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not REALTIME_ESTIMATOR:\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    fig, axs = plt.subplots(ncols=3, nrows=3, figsize=(15, 15))\n",
    "\n",
    "    i = 0\n",
    "    areaplot = subpools.groupby(['datetime','address']).sum().\\\n",
    "                reset_index().pivot(index='datetime', columns='address', values='usd_balance')\n",
    "    # deterministically color code the regions of the plot for visual inspection between weeks\n",
    "    areaplot = areaplot[list(areaplot.sum().sort_values(ascending=False).index)]\n",
    "    pal = ListedColormap(list(map(lambda x: '#'+x[20:26], list(areaplot.columns))))\n",
    "    areaplot.plot.area(legend=False, ax=axs.flat[i], title='USD balance by pool', colormap=pal)\n",
    "\n",
    "    i += 1\n",
    "    areaplot = subpools.groupby(['datetime','address']).sum().\\\n",
    "                reset_index().pivot(index='datetime', columns='address', values='finalAdjustedLiquidity')\n",
    "    areaplot = areaplot[list(areaplot.sum().sort_values(ascending=False).index)]\n",
    "    pal = ListedColormap(list(map(lambda x: '#'+x[20:26], list(areaplot.columns))))\n",
    "    areaplot.plot.area(legend=False, ax=axs.flat[i], title='Adjusted liquidity by pool', colormap=pal)\n",
    "\n",
    "    i += 1\n",
    "    areaplot = subpools.groupby(['datetime','address']).sum().\\\n",
    "                reset_index().pivot(index='datetime', columns='address', values='BAL_mined')\n",
    "    areaplot = areaplot[list(areaplot.sum().sort_values(ascending=False).index)]\n",
    "    pal = ListedColormap(list(map(lambda x: '#'+x[20:26], list(areaplot.columns))))\n",
    "    areaplot.plot.area(legend=False, ax=axs.flat[i], title='BAL mined by pool', colormap=pal)\n",
    "\n",
    "    i += 1\n",
    "    areaplot = subpools.groupby(['datetime','token_address']).sum().\\\n",
    "                reset_index().pivot(index='datetime', columns='token_address', values='usd_balance')\n",
    "    # deterministically color code the regions of the plot for visual inspection between weeks\n",
    "    areaplot = areaplot[list(areaplot.sum().sort_values(ascending=False).index)]\n",
    "    pal = ListedColormap(list(map(lambda x: '#'+x[20:26], list(areaplot.columns))))\n",
    "    areaplot.plot.area(legend=False, ax=axs.flat[i], title='USD balance by token', colormap=pal)\n",
    "\n",
    "    i += 1\n",
    "    areaplot = subpools.groupby(['datetime','token_address']).sum().\\\n",
    "                reset_index().pivot(index='datetime', columns='token_address', values='finalAdjustedLiquidity')\n",
    "    areaplot = areaplot[list(areaplot.sum().sort_values(ascending=False).index)]\n",
    "    pal = ListedColormap(list(map(lambda x: '#'+x[20:26], list(areaplot.columns))))\n",
    "    areaplot.plot.area(legend=False, ax=axs.flat[i], title='Adjusted liquidity by token', colormap=pal)\n",
    "\n",
    "    i += 1\n",
    "    areaplot = subpools.groupby(['datetime','token_address']).sum().\\\n",
    "                reset_index().pivot(index='datetime', columns='token_address', values='BAL_mined')\n",
    "    areaplot = areaplot[list(areaplot.sum().sort_values(ascending=False).index)]\n",
    "    pal = ListedColormap(list(map(lambda x: '#'+x[20:26], list(areaplot.columns))))\n",
    "    areaplot.plot.area(legend=False, ax=axs.flat[i], title='BAL mined by token', colormap=pal)\n",
    "\n",
    "    i += 3\n",
    "    areaplot = bal_mined.groupby(['block_number','chksum_bpt_holder']).sum().\\\n",
    "                reset_index().pivot(index='block_number', columns='chksum_bpt_holder', values='bal_mined')\n",
    "    areaplot = areaplot[list(areaplot.sum().sort_values(ascending=False).index)]\n",
    "    pal = ListedColormap(list(map(lambda x: '#'+x[20:26], list(areaplot.columns))))\n",
    "    areaplot.plot.area(legend=False, ax=axs.flat[i], title='BAL mined by LP', colormap=pal);\n",
    "    axs.flat[i].ticklabel_format(axis='x', style='plain')\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gov_factor > 1:\n",
    "    pregov = pd.DataFrame(totals_pre_govfactor)\n",
    "    pregov.columns= ['pregov']\n",
    "    postgov = pd.DataFrame(totals_post_govfactor)\n",
    "    postgov.columns= ['postgov']\n",
    "    compare = pregov.join(postgov)\n",
    "    compare['ratio'] = compare['postgov']/compare['pregov']\n",
    "    compare['voter'] = compare.index.isin(voters)\n",
    "    compare['color'] = compare['voter'].apply(lambda x: 'blue' if x else 'red')\n",
    "    effective_gov_factor = compare[compare['voter']]['ratio'].median()\n",
    "    effective_gov_loss = compare[~compare['voter']]['ratio'].median()\n",
    "    print(f'Effective govFactor: {effective_gov_factor:.3f} vs {effective_gov_loss:.3f}')\n",
    "    absolute_increase = (compare[compare['voter']]['postgov'] - compare[compare['voter']]['pregov']).sum()\n",
    "    print(f'Additional BAL mined by governors: {absolute_increase:.3f}')    \n",
    "    if not REALTIME_ESTIMATOR:\n",
    "        ax = compare[compare['voter']].plot.scatter(x='pregov', y='postgov', \n",
    "                                                    c='color', figsize=(10,10),\n",
    "                                                    label='voted'\n",
    "                                                   )\n",
    "        compare[~compare['voter']].plot.scatter(x='pregov', y='postgov', \n",
    "                                                    c='color', figsize=(10,10),\n",
    "                                                    label='abstained', ax=ax\n",
    "                                                   )\n",
    "\n",
    "        x = compare['pregov']\n",
    "        y = compare['pregov']\n",
    "        ax.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)), \n",
    "                color='grey', linestyle='--',\n",
    "                label = 'if no govFactor')\n",
    "\n",
    "        x = compare[compare['voter']]['pregov']\n",
    "        y = compare[compare['voter']]['postgov']\n",
    "        ax.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)), \n",
    "                color='lightblue', linestyle='--',\n",
    "                label = 'w/ govFactor (voters)')\n",
    "        \n",
    "        x = compare[~compare['voter']]['pregov']\n",
    "        y = compare[~compare['voter']]['postgov']\n",
    "        ax.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)), \n",
    "                color='salmon', linestyle='--',\n",
    "                label = 'w/ govFactor (abstained)')\n",
    "\n",
    "\n",
    "        ax.set_xlabel('BAL earned pre-govFactor')\n",
    "        ax.set_ylabel('BAL earned post-govFactor')\n",
    "        ax.set_title('Effects of govFactor on each liquidity provider')\n",
    "        ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not REALTIME_ESTIMATOR:\n",
    "    print('Final Check Totals')\n",
    "    _lm = pd.read_json(reports_dir+'/_totalsLiquidityMining.json', orient='index').sum().values[0]\n",
    "    _claim = pd.read_json(reports_dir+'/_totals.json', orient='index').sum().values[0]\n",
    "    print(f'Liquidity Mining: {format(_lm, f\".{CLAIM_PRECISION}f\")}')\n",
    "    print(f'Gas Reimbursement: {format(_claim-145000, f\".{CLAIM_PRECISION}f\")}')\n",
    "    print(f'Total: {format(_claim, f\".{CLAIM_PRECISION}f\")}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
