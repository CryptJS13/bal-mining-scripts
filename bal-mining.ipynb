{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "# Google BigQuery SQL to get the blocks mined around a timestamp\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "# SELECT * FROM `bigquery-public-data.crypto_ethereum.blocks`\n",
    "# WHERE timestamp > \"2021-01-24 23:59:30\"\n",
    "# and timestamp < \"2021-01-25 00:00:30\"\n",
    "# order by timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "REALTIME_ESTIMATOR = True\n",
    "# set the window of blocks, will be overwritten if REALTIME_ESTIMATOR == True\n",
    "WEEK = 34\n",
    "START_BLOCK = 11675866\n",
    "END_BLOCK = 11721455\n",
    "# we can hard code latest gov proposal if we want\n",
    "latest_gov_proposal = ''\n",
    "latest_gov_proposal_snapshot_block = 11655773\n",
    "gov_factor = 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-28 09:51:23\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-c0d7b182294e>:6: UserWarning: Running realtime estimator\n",
      "  warnings.warn('Running realtime estimator')\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "import warnings\n",
    "\n",
    "\n",
    "if REALTIME_ESTIMATOR:\n",
    "    warnings.warn('Running realtime estimator')\n",
    "    \n",
    "    from urllib.request import urlopen\n",
    "    import json\n",
    "    url = 'https://ipfs.fleek.co/ipns/balancer-team-bucket.storage.fleek.co/balancer-claim/snapshot'\n",
    "    jsonurl = urlopen(url)\n",
    "    claims = json.loads(jsonurl.read())\n",
    "    claimable_weeks = [20+int(w) for w in claims.keys()]\n",
    "    most_recent_week = max(claimable_weeks)\n",
    "    # delete the estimates for the most recent published week, since now there's an official value available on IPFS\n",
    "    project_id = os.environ['GCP_PROJECT']\n",
    "    sql = f'''\n",
    "        DELETE FROM {project_id}.bal_mining_estimates.pool_estimates\n",
    "        WHERE week = {most_recent_week}\n",
    "    '''\n",
    "    client = bigquery.Client()\n",
    "    query = client.query(sql)\n",
    "    query.result()\n",
    "    sql = f'''\n",
    "        DELETE FROM {project_id}.bal_mining_estimates.lp_estimates\n",
    "        WHERE week = {most_recent_week}\n",
    "    '''\n",
    "    client = bigquery.Client()\n",
    "    query = client.query(sql)\n",
    "    query.result()\n",
    "    \n",
    "    \n",
    "    from datetime import datetime\n",
    "    week_1_start = '01/06/2020 00:00:00 UTC'\n",
    "    week_1_start = datetime.strptime(week_1_start, '%d/%m/%Y %H:%M:%S %Z')\n",
    "    WEEK = int(1 + (datetime.utcnow() - week_1_start).days/7)  # this is what week we're actually in\n",
    "    \n",
    "    # get all blocks of the first hour of this week to determine the first block of the week\n",
    "    sql = '''\n",
    "    SELECT number FROM `bigquery-public-data.crypto_ethereum.blocks`\n",
    "    where timestamp >= TIMESTAMP_TRUNC(CURRENT_TIMESTAMP(), WEEK(MONDAY))\n",
    "    and timestamp <= TIMESTAMP_ADD(TIMESTAMP_TRUNC(CURRENT_TIMESTAMP(), WEEK(MONDAY)), interval 1 HOUR)\n",
    "    order by timestamp\n",
    "    limit 1\n",
    "    '''\n",
    "    results = bigquery.Client().query(sql).result()\n",
    "    for row in results:\n",
    "        START_BLOCK = row.number\n",
    "\n",
    "    sql = '''\n",
    "    SELECT MAX(number) as number FROM `bigquery-public-data.crypto_ethereum.blocks`\n",
    "    '''\n",
    "    results = bigquery.Client().query(sql).result()\n",
    "    for row in results:\n",
    "        END_BLOCK = row.number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Consider installing rusty-rlp to improve pyrlp performance with a rust based backend\n"
     ]
    }
   ],
   "source": [
    "from web3 import Web3\n",
    "import os\n",
    "web3_provider = os.environ['ENDPOINT_URL']\n",
    "w3 = Web3(Web3.WebsocketProvider(web3_provider))\n",
    "start_block_timestamp = w3.eth.getBlock(START_BLOCK).timestamp\n",
    "end_block_timestamp = w3.eth.getBlock(END_BLOCK).timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most recent governance proposal: QmRVELW33aicrnyL16EM18spDJeRzzpM6NZv469XxAXffx\n",
      "Title: [Proposal] Increase GNO cap from Cap3 to Cap4 ($10M -> $30M)\n",
      "Most recent governance proposal: QmfJWL3Di2CCzWcH1A3YCDsmZhxn3cgeiavBnNWEeopzAg\n",
      "Title: [Proposal] Balancer Exchange Gas Reimbursement\n",
      "Most recent governance proposal: QmXQpKyw1BvYgZtvC2KGqrDezWBfSUBEKK77Kx866yBLYf\n",
      "Title: [Proposal] SourceCred Engagement Incentives\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# the realtime estimator overrides the hard coded gov proposal\n",
    "if REALTIME_ESTIMATOR:\n",
    "    latest_gov_proposal = ''\n",
    "\n",
    "# if not explicitly set, try to get the latest gov proposal from the snapshot API\n",
    "if gov_factor > 1 and not latest_gov_proposal:\n",
    "    space_url = 'https://hub.snapshot.page/api/spaces/balancer'\n",
    "    members = json.loads(requests.get(space_url).content).get('members')\n",
    "\n",
    "    proposals_url = 'https://hub.snapshot.page/api/balancer/proposals'\n",
    "    proposals = pd.DataFrame(json.loads(requests.get(proposals_url).content)).transpose()\n",
    "    # only official proposals count\n",
    "    proposals = proposals[proposals['address'].isin(members)]\n",
    "    proposals['end'] = proposals['msg'].apply(lambda x: x['payload'].get('end',0))\n",
    "    # only proposals that ended before the beginning of the current week count\n",
    "    proposals = proposals[proposals['end'] < start_block_timestamp]\n",
    "    latest_gov_proposals = proposals[proposals['end']==proposals['end'].max()]\n",
    "#     latest_gov_proposal_snapshot_block = proposals.loc[latest_gov_proposal,'msg']['payload'].get('snapshot',0)\n",
    "#     title = proposals.loc[latest_gov_proposal,'msg']['payload'].get('name',0)\n",
    "    for i,p in latest_gov_proposals.iterrows():\n",
    "        print(f'Most recent governance proposal: {i}')\n",
    "        print(f\"Title: {p['msg']['payload'].get('name',0)}\")\n",
    "#     if latest_gov_proposal == 'QmWtk4pJ2CNhmLFK9Nhk5gGD7xr2KbsK4cfpFXyR9YCbYG': # if the latest proposal is the govFactor proposal\n",
    "#         gov_factor = 1\n",
    "#         print(f'No proposals since the govFactor proposal until the beginning of the week. Using govFactor=1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of tokens eligible for liquidity mining\n",
    "if REALTIME_ESTIMATOR:\n",
    "    whitelist_df = pd.read_json('https://raw.githubusercontent.com/balancer-labs/assets/master/lists/eligible.json')\n",
    "else:\n",
    "    whitelist_df = pd.read_json(f'https://raw.githubusercontent.com/balancer-labs/assets/w{WEEK}/lists/eligible.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import json5 as json\n",
    "if REALTIME_ESTIMATOR:\n",
    "    url = 'https://raw.githubusercontent.com/balancer-labs/bal-mining-scripts/master/config/blacklisted_sharelholders.json'\n",
    "    jsonurl = urlopen(url)\n",
    "    BLACKLISTED_SHAREHOLDERS = json.loads(jsonurl.read())['address']\n",
    "\n",
    "    url = 'https://raw.githubusercontent.com/balancer-labs/bal-mining-scripts/master/config/equivalent_sets.json5'\n",
    "    jsonurl = urlopen(url)\n",
    "    EQUIVALENT_SETS = json.loads(jsonurl.read())['sets']\n",
    "else:\n",
    "    BLACKLISTED_SHAREHOLDERS = json.load(open('config/blacklisted_sharelholders.json'))['address']\n",
    "    EQUIVALENT_SETS = json.load(open('config/equivalent_sets.json5'))['sets']\n",
    "    \n",
    "BLACKLISTED_SHAREHOLDERS_lower = [x.lower() for x in BLACKLISTED_SHAREHOLDERS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAL_TOKEN = '0xba100000625a3754423978a60c9317c58a424e3D'\n",
    "SNAPSHOT_WINDOW_SIZE = 256\n",
    "CLAIM_PRECISION = 18 # leave out of results addresses that mined less than CLAIM_THRESHOLD BAL\n",
    "CLAIM_THRESHOLD = 10**(-CLAIM_PRECISION)\n",
    "WEEKLY_MINED = 145000\n",
    "LIQUIDITY_STAKING = 45000\n",
    "if REALTIME_ESTIMATOR:\n",
    "    week_passed = (end_block_timestamp - start_block_timestamp)/(7*24*3600)\n",
    "    WEEKLY_MINED = int(WEEKLY_MINED*week_passed)\n",
    "    LIQUIDITY_STAKING = int(LIQUIDITY_STAKING*week_passed)\n",
    "STAKERS_SHARE = LIQUIDITY_STAKING / WEEKLY_MINED\n",
    "reports_dir = f'reports/{WEEK}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(reports_dir):\n",
    "    os.mkdir(reports_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "flatten_list = lambda t: [item for sublist in t for item in sublist]\n",
    "\n",
    "def get_wrap_factor_of_pair(a, b):\n",
    "    a = Web3.toChecksumAddress(a)\n",
    "    b = Web3.toChecksumAddress(b)\n",
    "    for soft in EQUIVALENT_SETS:\n",
    "        for hard in soft:\n",
    "            if a in hard and b in hard:\n",
    "                return .1\n",
    "        flat = flatten_list(soft)\n",
    "        if a in flat and b in flat:\n",
    "            return .2\n",
    "    return 1\n",
    "\n",
    "\n",
    "def get_wrap_factor(tokens_weights):\n",
    "    length = len(tokens_weights)\n",
    "    shape = (length, length)\n",
    "    wf_matrix = np.zeros(shape)\n",
    "    tokens = tokens_weights.index.get_level_values('token_address')\n",
    "    for i in range(len(tokens_weights)-1):\n",
    "        for j in range(i+1, len(tokens_weights)):\n",
    "            wf_matrix[i,j] = wf_matrix[i,j] + get_wrap_factor_of_pair(tokens[i],tokens[j])\n",
    "    weights_vector = np.array(tokens_weights)\n",
    "    weights_matrix = np.outer(weights_vector,weights_vector.T)\n",
    "    try:\n",
    "        element_wise_product = weights_matrix * wf_matrix\n",
    "    except:\n",
    "        print(tokens_weights)\n",
    "        print(weights_matrix)\n",
    "        print(wf_matrix)\n",
    "        raise\n",
    "    return element_wise_product.sum()/np.triu(weights_matrix, k=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_staking_boosts_of_pair(a, b, bal_multiplier=1):\n",
    "    a = Web3.toChecksumAddress(a)\n",
    "    b = Web3.toChecksumAddress(b)\n",
    "    if a==BAL_TOKEN:\n",
    "        cap_b = whitelist_df.loc[b.lower(),'cap']\n",
    "        if cap_b == np.inf:\n",
    "            return (bal_multiplier, 1)\n",
    "        else:\n",
    "            return (1, 1)\n",
    "    elif b==BAL_TOKEN:\n",
    "        cap_a = whitelist_df.loc[a.lower(),'cap']\n",
    "        if cap_a == np.inf:\n",
    "            return (1, bal_multiplier)\n",
    "        else:\n",
    "            return (1, 1)\n",
    "    else:\n",
    "        return (1, 1)\n",
    "\n",
    "\n",
    "def get_BRF(tokens_weights, bal_multiplier=1):\n",
    "    if type(bal_multiplier) != int: # expect a series of BAL multipliers on the third pass\n",
    "        block_number = tokens_weights.index.get_level_values('block_number').drop_duplicates().values\n",
    "        if len(block_number)>1:\n",
    "            raise Exception('got more than one block_number {}'.format(block_number))\n",
    "        else:\n",
    "            bal_multiplier = final_bal_multiplier[block_number[0]]\n",
    "            \n",
    "    if 'shareholders_subpool' in tokens_weights.index.names:\n",
    "        if tokens_weights.index.get_level_values('shareholders_subpool').all():\n",
    "            bal_multiplier=1\n",
    "    else:\n",
    "        if tokens_weights.index.nlevels==4:\n",
    "            raise Exception('shareholders_subpool not in index')    \n",
    "            \n",
    "    denominator = 0\n",
    "    numerator = 0\n",
    "    token_address = list(tokens_weights.index.get_level_values('token_address'))\n",
    "    token_weights = list(tokens_weights)\n",
    "    for i in range(len(tokens_weights)-1):\n",
    "        for j in range(i+1, len(tokens_weights)):\n",
    "            token_A = token_address[i]\n",
    "            token_B = token_address[j]\n",
    "            weight_A = token_weights[i]\n",
    "            weight_B = token_weights[j]\n",
    "            staking_boosts = get_staking_boosts_of_pair(token_A, token_B, bal_multiplier)\n",
    "            staking_boost_A = staking_boosts[0]\n",
    "            staking_boost_B = staking_boosts[1]\n",
    "            staking_boost_of_pair = (staking_boost_A * weight_A + staking_boost_B * weight_B) / (weight_A + weight_B)\n",
    "            \n",
    "            ratio_factor = 4 * (weight_A / (weight_A + weight_B)) * (weight_B / (weight_A + weight_B))\n",
    "            \n",
    "            numerator += staking_boost_of_pair * ratio_factor * weight_A * weight_B\n",
    "            denominator += weight_A * weight_B\n",
    "    return numerator/denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.facecolor'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ebeaea8762e41a995aaf29052a2bebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Getting snapshot timestamps'), FloatProgress(value=0.0, max=90.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "week 35: 90 snapshot blocks\n",
      "week 35: first snapshot block: 11721598 ([11721598, 11721854, 11722110]...)\n",
      "week 35: last snapshot block: 11744382 (...[11743870, 11744126, 11744382])\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "def get_list_of_snapshot_blocks(start, end):\n",
    "    block_list = range(end, start, -SNAPSHOT_WINDOW_SIZE)\n",
    "    block_list = list(block_list)\n",
    "    block_list.sort()\n",
    "    return block_list\n",
    "\n",
    "snapshot_blocks = get_list_of_snapshot_blocks(START_BLOCK, END_BLOCK)\n",
    "snapshot_timestamps_blocks = {w3.eth.getBlock(b).timestamp: b \\\n",
    "                              for b in tqdm(snapshot_blocks, 'Getting snapshot timestamps')}\n",
    "snapshot_blocks_timestamps = {v: k for k,v in snapshot_timestamps_blocks.items()}\n",
    "snapshot_blocks_as_str = [str(b) for b in snapshot_blocks]\n",
    "print('week {}: {} snapshot blocks'.format(WEEK, len(snapshot_blocks)))\n",
    "print('week {}: first snapshot block: {} ({}...)'.format(WEEK, min(snapshot_blocks), snapshot_blocks[:3]))\n",
    "print('week {}: last snapshot block: {} (...{})'.format(WEEK, max(snapshot_blocks), snapshot_blocks[-3:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove kovan\n",
    "whitelist_df.drop(columns=['kovan'], inplace=True)\n",
    "whitelist_df.dropna(inplace=True)\n",
    "# convert string to cap\n",
    "whitelist_df.rename(columns={'homestead':'cap'}, inplace=True)\n",
    "CAP_TIERS = {\n",
    "    'cap1':   1e6,\n",
    "    'cap2':   3e6,\n",
    "    'cap3':  10e6,\n",
    "    'cap4':  30e6,\n",
    "    'cap5': 100e6,\n",
    "    'uncapped': np.inf,\n",
    "}\n",
    "whitelist_df['cap'] = whitelist_df['cap'].apply(lambda x: CAP_TIERS[x])\n",
    "# lower case the token addresses for later join\n",
    "whitelist_df.index.name = 'checksum_token_address'\n",
    "whitelist_df.reset_index(inplace=True)\n",
    "whitelist_df['checksum_token_address'] = whitelist_df['checksum_token_address'].apply(Web3.toChecksumAddress)\n",
    "whitelist_df.set_index(whitelist_df['checksum_token_address'].str.lower(), inplace=True)\n",
    "whitelist_df.index.name = 'token_address'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-28 09:52:03 - Getting token decimals from ethereum node...\n",
      "0xE0B7927c4aF23765Cb51314A0E0521A9645F0E2A: decimals() returned bad output, assuming 0\n",
      "2021-01-28 09:53:17 - Done!\n"
     ]
    }
   ],
   "source": [
    "# get decimals of whitelisted tokens\n",
    "import json\n",
    "from web3.exceptions import ABIFunctionNotFound, BadFunctionCallOutput, InvalidAddress\n",
    "token_abi = json.load(open('abi/BToken.json'))\n",
    "def get_token_decimals(token_address):\n",
    "    try:\n",
    "        token_contract = w3.eth.contract(token_address, abi=token_abi)\n",
    "    except InvalidAddress:\n",
    "        print('Invalid Address: ' + token_address)\n",
    "        raise\n",
    "    try:\n",
    "        return token_contract.functions.decimals().call()\n",
    "    except ABIFunctionNotFound:\n",
    "        print(f'{token_address} does not implement decimals(), assuming 0')\n",
    "        return 0\n",
    "    except BadFunctionCallOutput:\n",
    "        print(f'{token_address}: decimals() returned bad output, assuming 0')\n",
    "        return 0\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Getting token decimals from ethereum node...')\n",
    "whitelist_df['decimals'] = whitelist_df['checksum_token_address'].apply(get_token_decimals)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43149716fd7041a48239967eaa0cfee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Getting prices'), FloatProgress(value=0.0, max=400.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# get USD prices of whitelist tokens\n",
    "import requests\n",
    "from time import sleep\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "\n",
    "\n",
    "if REALTIME_ESTIMATOR:\n",
    "    prices_json = {}\n",
    "else:\n",
    "    try:\n",
    "        prices_json = json.load(open(reports_dir+'/_prices.json'))\n",
    "    except FileNotFoundError:\n",
    "        prices_json = {}\n",
    "\n",
    "MARKET_API_URL = 'https://api.coingecko.com/api/v3'\n",
    "price_query = MARKET_API_URL+'/coins/ethereum/contract/{}/market_chart/range?&vs_currency=usd&from={}&to={}'\n",
    "time_bounded_price_query = price_query.format('{}', start_block_timestamp, end_block_timestamp)\n",
    "whitelist_df['prices_api_response'] = ''\n",
    "whitelist_df['prices_dict'] = ''\n",
    "for i in tqdm(whitelist_df.index, 'Getting prices'):\n",
    "    checksum_token_address = whitelist_df.loc[i,'checksum_token_address']\n",
    "    query_url = time_bounded_price_query.format(checksum_token_address)\n",
    "    # when running week 26 we ran across a bug in Coingecko price feed where the price of WETH was reported as 0\n",
    "    # price of ETH is unaffected, so using it instead\n",
    "    if '0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2' in query_url:\n",
    "        query_url = query_url.replace('/contract/0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2','')\n",
    "    prices_dict = {'prices': prices_json.get(checksum_token_address, None)}\n",
    "    if prices_dict['prices'] is None:\n",
    "        prices_dict = None\n",
    "    tries = 0\n",
    "    while prices_dict is None:\n",
    "        token_prices = requests.get(query_url)\n",
    "        try:\n",
    "            prices_dict = json.loads(token_prices.content)\n",
    "        except:\n",
    "            pass\n",
    "        # sleep for one second to avoid being blocked\n",
    "        sleep(1)\n",
    "        tries += 1\n",
    "        if tries > 5:\n",
    "            break\n",
    "        whitelist_df.loc[i,'prices_api_response'] = token_prices.content\n",
    "    whitelist_df.loc[i,'prices_dict'] = [prices_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens not found in Coingecko: ['0x0c6f5f7d555e7518f6841a79436bd2b1eef03381']\n",
      "Prices not found in Coingecko for: ['0x9a48bd0ec040ea4f1d3147c025cd4076a2e71e3e', '0x3a3a65aab0dd2a17e3f1947ba16138cd37d08c04', '0x7d2d3688df45ce7c552e19c27e007673da9204b8', '0x71010a9d003445ac60c4e6a7017c1e89a477b438', '0x54355ae0485f9420e6ce4c00c10172dc8e5728a3', '0x07509c281b55a1675d3f71f1c4ab67829eb731d3', '0x0bf54992649c19bd8db4080078a32383827352f3', '0x48ac44f4e29e602f851b84c271c22b85b9447251', '0xc7088fac73c55bfae5c2a963c3029b072c7dff25', '0xe6404a4472e5222b440f8fafb795553046000841', '0xc39835d32428728cbde6903f84c76750976c0323', '0x8abf3a95862619a55fa00cb3e4eedbe113ff468c', '0x2409d6059e2a8130c099e49f3cb418fd6c3d9aff', '0xd218d75ba0fc45858a4e9ef57a257ed9977db5f4', '0x253444bd9ecf11e5516d6d00974e91c9f0857ccb', '0x78481fb80caabb252909218164266ac83f815000', '0x7e4d1cd8927ce41bcbfa4f32cada1a6998cb5a51', '0xc19216eea17b2f4dd677f1024cda59c7d142f189', '0x1003ec54f51565ff86ac611184ea23d6310cae71', '0xabc754ac2161b557d28062f41dcc0fc18440ac7e', '0x1bcca39ae82e53dede8ec5500c3bcd76cd1e0072', '0xa12a696b9b11788076a6cb384cac6986b82545e1', '0xb1ca7e6714263a64659a3a89e1c313af30fd660a', '0x2bf417fda6e73b8ea605df0f33ad029f8d4b795a', '0x1ce9200c98b6d9999b60bff53860475a993a8b68', '0xffee21b4bb7084a9416205544101ae9f472c7159', '0xcae169afde69f297c7817ed5f4a6816c0e38137d', '0x654424f4b3ed6de828c9ca30484dc1a626bb5fba', '0xdbf5c7d8ac5007667617a15db2c1b1d616c9d302', '0x5cd487ce4db7091292f2e914f7b31445bd4a5e1b', '0xac1565e473f69fada09661a6b4103fbbf801ceee', '0xb32c960c46f28059c2b5f1c3ecc2b9dd77ab0aa0', '0x8a63be90f095f6777be3ed25d9fc7cd2a63ddb30', '0x621e3b71d07b51242bcca167928e184235a4bb87', '0x77b1465b0e01ba085e515324e30fee6555c623ea', '0x8e4dbf540bf814c044785218b58c930b20a56be1', '0x8ddf05c42c698329053c4f39b5bb05a350fd8132', '0x81ab848898b5ffd3354dbbefb333d5d183eedcb5', '0x29e9fdf5933824ad21bc6dbb8bf156efa3735e32', '0x89e3ac6dd69c15e9223be7649025d6f68dab1d6a', '0x45f24baeef268bb6d63aee5129015d69702bcdfa', '0xb2fdd60ad80ca7ba89b9bab3b5336c2601c020b4', '0xbbe319b73744db9d54f5d29df7d8256b7e43995c', '0x208d174775dc39fe18b1b374972f77ddec6c0f73', '0x0cf58006b2400ebec3eb8c05b73170138a340563', '0xbc16da9df0a22f01a16bc0620a27e7d6d6488550', '0xdaff85b6f5787b2d9ee11ccdf5e852816063326a', '0xdea67845a51e24461d5fed8084e69b426af3d5db']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "tokens_not_found = whitelist_df[whitelist_df['prices_dict'].apply(lambda x: 'error' in x.keys())].index\n",
    "whitelist_df.drop(index=tokens_not_found, inplace=True)\n",
    "print('Tokens not found in Coingecko: {}'.format(list(tokens_not_found)))\n",
    "\n",
    "whitelist_df['prices_lists'] = whitelist_df['prices_dict'].apply(lambda x: x.get('prices'))\n",
    "prices_not_found = whitelist_df[whitelist_df['prices_lists'].apply(lambda x: len(x)==0)].index\n",
    "whitelist_df.drop(index=prices_not_found, inplace=True)\n",
    "print('Prices not found in Coingecko for: {}'.format(list(prices_not_found)))\n",
    "\n",
    "exploded_whitelist_df = whitelist_df.explode('prices_lists').dropna()\n",
    "exploded_whitelist_df.reset_index(inplace=True)\n",
    "exploded_whitelist_df[['timestamp','price']] = pd.DataFrame(exploded_whitelist_df.prices_lists.tolist(), index=exploded_whitelist_df.index)\n",
    "\n",
    "prices_df = exploded_whitelist_df[['token_address', 'checksum_token_address', 'cap', 'timestamp', 'price']].copy()\n",
    "\n",
    "prices_df['ts_price'] = prices_df.apply(lambda x: [x['timestamp'], x['price']], axis=1)\n",
    "\n",
    "if not REALTIME_ESTIMATOR:\n",
    "    prices_df.groupby('checksum_token_address').agg(list)['ts_price'].to_json(reports_dir+'/_prices.json',\n",
    "                                                                 orient='index',\n",
    "                                                                 indent=4\n",
    "                                                                )\n",
    "\n",
    "prices_df['timestamp'] = prices_df['timestamp']//1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get eligible token balances of every balancer pool at every snapshot block from Big Query\n",
    "get_pools_sql = '''\n",
    "SELECT pool FROM `blockchain-etl.ethereum_balancer.BFactory_event_LOG_NEW_POOL` \n",
    "'''\n",
    "\n",
    "sql = \"\"\"\n",
    "select * from `blockchain-etl.ethereum_balancer.view_token_balances_subset`\n",
    "where token_address in (\\'{0}\\')\n",
    "and address in ({1})\n",
    "and token_address not in ('0xd46ba6d942050d489dbd938a2c909a5d5039a161')\n",
    "and block_number in ({2})\n",
    "and balance > 0\n",
    "\n",
    "union all\n",
    "\n",
    "select '0xd46ba6d942050d489dbd938a2c909a5d5039a161' as token_address,\n",
    "* from `blockchain-etl.ethereum_balancer.view_token_balances_subset_AMPL`\n",
    "where address in ({1})\n",
    "and block_number in ({2})\n",
    "and balance > 0\n",
    "\n",
    "\"\"\".format('\\',\\''.join(whitelist_df.index), # only get balances of tokens for which there is a price feed\n",
    "           get_pools_sql, \n",
    "           ','.join(snapshot_blocks_as_str))\n",
    "# print(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-28 10:03:13 - Querying BigQuery...\n",
      "2021-01-28 10:04:19 - Done (362537 records)\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud import bigquery_storage\n",
    "import time\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Querying BigQuery...')\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "bqstorageclient = bigquery_storage.BigQueryReadClient()\n",
    "pools_balances = (\n",
    "    client.query(sql)\n",
    "    .result()\n",
    "    .to_dataframe(bqstorage_client=bqstorageclient)\n",
    ")\n",
    "n = len(pools_balances)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + f' - Done ({n} records)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pools_balances['scaled_balance'] = pools_balances['balance'] * pools_balances.join(whitelist_df['decimals'], on='token_address')['decimals'].apply(lambda x: 10**(-x))\n",
    "pools_balances['timestamp'] = pools_balances['block_number'].apply(lambda x: snapshot_blocks_timestamps[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pools_balances.set_index(['address','block_number','token_address'], inplace=True)\n",
    "number_of_liquid_eligible_tokens = pools_balances.groupby(['address','block_number']).size()\n",
    "number_of_liquid_eligible_tokens.name = 'number_of_liquid_eligible_tokens'\n",
    "pools_balances = pools_balances.join(number_of_liquid_eligible_tokens)\n",
    "eligible_pools_balances = pools_balances[pools_balances['number_of_liquid_eligible_tokens']>=2].copy()\n",
    "eligible_pools_balances.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge balances and prices datasets on nearest timestamp, and compute USD balance of each token in each pool at each block\n",
    "usd_pools_balances = pd.merge_asof(eligible_pools_balances.sort_values(by='timestamp'), \n",
    "                                   prices_df.sort_values(by='timestamp'), \n",
    "                                   on='timestamp', by='token_address', direction='nearest')\n",
    "usd_pools_balances['usd_balance'] = usd_pools_balances['scaled_balance'] * usd_pools_balances['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get token weights and swap fees of pools with public swap enabled\n",
    "sql = \"\"\"\n",
    "select W.*, COALESCE(swapFee,'1000000000000') as swapfee\n",
    "from `blockchain-etl.ethereum_balancer.view_pools_tokens_denorm_weights` W\n",
    "left join `blockchain-etl.ethereum_balancer.view_pools_fees` F\n",
    "on W.address = F.address and W.block_number = F.block_number\n",
    "left join `blockchain-etl.ethereum_balancer.view_pools_public_swaps` S\n",
    "on W.address = S.address and W.block_number = S.block_number\n",
    "where COALESCE(public_,'false') = 'true'\n",
    "and W.denorm > 0\n",
    "and token_address in (\\'{}\\')\n",
    "and W.address in ('{}')\n",
    "and W.block_number in ({})\n",
    "\"\"\".format('\\',\\''.join(whitelist_df.index), # only get weights of tokens for which there is a price feed\n",
    "           '\\',\\''.join(eligible_pools_balances['address'].drop_duplicates()), \n",
    "           ','.join(snapshot_blocks_as_str))\n",
    "# print(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-28 10:04:23 - Querying BigQuery...\n",
      "2021-01-28 10:07:10 - Done (305456 records)\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud import bigquery_storage\n",
    "import time\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Querying BigQuery...')\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "bqstorageclient = bigquery_storage.BigQueryReadClient()\n",
    "pools_weights = (\n",
    "    client.query(sql)\n",
    "    .result()\n",
    "    .to_dataframe(bqstorage_client=bqstorageclient)\n",
    ")\n",
    "n = len(pools_weights)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + f' - Done ({n} records)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the merge removes records associated with balances of tokens that are not part of the pool\n",
    "pools_weights_balances = pools_weights.merge(usd_pools_balances, \n",
    "                                             on=['address', 'token_address', 'block_number'],\n",
    "                                             how='inner')\n",
    "pools_weights_balances.set_index(['address', 'token_address', 'block_number'], inplace=True)\n",
    "pools_weights_balances['denorm'] = pools_weights_balances['denorm'].apply(float)\n",
    "summed_weights = pools_weights_balances['denorm'].groupby(['address','block_number']).sum()\n",
    "norm_weights = pools_weights_balances['denorm'] / summed_weights\n",
    "norm_weights.name = 'norm_weights'\n",
    "pools_weights_balances = pools_weights_balances.join(norm_weights)\n",
    "# remove pools with only one eligible token\n",
    "pools_weights_balances = pools_weights_balances[pools_weights_balances['norm_weights']<1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-28 10:07:12 - Computing wrap factor...\n",
      "2021-01-28 10:08:44 - Done\n"
     ]
    }
   ],
   "source": [
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Computing wrap factor...')\n",
    "wrap_factor = pools_weights_balances['norm_weights'].\\\n",
    "                    groupby(['address','block_number']).\\\n",
    "                    agg(get_wrap_factor)\n",
    "wrap_factor.name = 'wrap_factor'\n",
    "pools_weights_balances = pools_weights_balances.join(wrap_factor)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-28 10:08:44 - Computing BRF (first pass)...\n",
      "2021-01-28 10:10:09 - Done\n"
     ]
    }
   ],
   "source": [
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Computing BRF (first pass)...')\n",
    "brf = pools_weights_balances['norm_weights']. \\\n",
    "                groupby(['address','block_number']). \\\n",
    "                agg(get_BRF)\n",
    "brf.name = 'first_pass_brf'\n",
    "pools_weights_balances = pools_weights_balances.join(brf)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the fee factor\n",
    "# https://forum.balancer.finance/t/modifying-feefactor-toward-reducing-the-mining-penalty-for-high-fee-pools/103\n",
    "# a swapfee of 1% is stored in the smart contracts as 1e+16 (0.01e+18)\n",
    "# fee factor formula as defined in the specs above takes as argument the fee as a percentage (eg 1 for a fee of 1% - not 0.01)\n",
    "pools_weights_balances['fee_factor'] = np.exp(-(0.25 * \\\n",
    "                                                (100 * \\\n",
    "                                                 (pools_weights_balances['swapfee'].astype(float) / 1E18)))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pools_weights_balances['adjustedLiquidityPreTokenCap'] = pools_weights_balances['usd_balance'] * \\\n",
    "                                                            pools_weights_balances['fee_factor'] * \\\n",
    "                                                            pools_weights_balances['wrap_factor'] * \\\n",
    "                                                            pools_weights_balances['first_pass_brf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the tokenCapFactor for each token_address at each block_number\n",
    "tokenCapFactor = np.minimum(1, whitelist_df['cap'] / (pools_weights_balances['adjustedLiquidityPreTokenCap'].\\\n",
    "    groupby(['block_number','token_address']).\\\n",
    "    sum().\\\n",
    "    sort_values()))\n",
    "tokenCapFactor.name = 'tokenCapFactor'\n",
    "pools_weights_balances = pools_weights_balances.join(tokenCapFactor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pools_weights_balances['token_capped_usd_balance'] = pools_weights_balances['usd_balance'] * \\\n",
    "                                                        pools_weights_balances['tokenCapFactor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get liquidity providers and the amount of BPT each has\n",
    "# private and smart pools don't have BPT, so we assign 1 fictitious BPT to the controller\n",
    "# BAL mined by controllers of smart pools created by the CRPFactory will be redistributed to the controller token holders later in the process\n",
    "sql = \"\"\"\n",
    "with shared_pools as (\n",
    "  select token_address as address, address as bpt_holder, block_number, balance as bpt from `blockchain-etl.ethereum_balancer.view_token_balances_subset`\n",
    "  where token_address in ('{0}')\n",
    "  and block_number in ({1})\n",
    "  and balance > 0\n",
    "),\n",
    "private_pools as (\n",
    "  select address, controller as bpt_holder, block_number, 1 as bpt from `blockchain-etl.ethereum_balancer.view_pools_controllers`\n",
    "  where address not in (select address from shared_pools)\n",
    "  and block_number in ({1})\n",
    ")\n",
    "select * from shared_pools\n",
    "union all\n",
    "select * from private_pools\n",
    "\"\"\".format('\\',\\''.join(pools_weights_balances.index.get_level_values('address').drop_duplicates()), \n",
    "           ','.join(snapshot_blocks_as_str))\n",
    "# print(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-28 10:10:10 - Querying BigQuery...\n",
      "2021-01-28 10:13:31 - Done (1315172 records)\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud import bigquery_storage\n",
    "import time\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Querying BigQuery...')\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "bqstorageclient = bigquery_storage.BigQueryReadClient()\n",
    "bpt_balances = (\n",
    "    client.query(sql)\n",
    "    .result()\n",
    "    .to_dataframe(bqstorage_client=bqstorageclient)\n",
    ")\n",
    "n = len(bpt_balances)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + f' - Done ({n} records)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_shareholder = bpt_balances['bpt_holder'].isin(BLACKLISTED_SHAREHOLDERS_lower)\n",
    "bpt_balances['is_shareholder'] = is_shareholder\n",
    "bpt_balances.set_index(['address','block_number','is_shareholder','bpt_holder'], inplace=True)\n",
    "bpt_balances.rename_axis(index={'is_shareholder': 'shareholders_subpool'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split pools that have a blacklisted shareholder as one of their LPs\n",
    "split_pools = bpt_balances['bpt'].groupby(['address','block_number','shareholders_subpool']).sum()\n",
    "total_bpt = bpt_balances['bpt'].groupby(['address','block_number']).sum()\n",
    "relative_size_of_subpool = split_pools/total_bpt\n",
    "relative_size_of_subpool.name = 'relative_size_of_subpool'\n",
    "subpools = pools_weights_balances.join(relative_size_of_subpool, how='inner')\n",
    "\n",
    "# recompute values according to the relative size of the subpool\n",
    "splitable_cols = ['balance', 'scaled_balance', 'usd_balance', 'adjustedLiquidityPreTokenCap', \n",
    "                'token_capped_usd_balance']\n",
    "for c in splitable_cols:\n",
    "    subpools[c] = subpools[c] * subpools['relative_size_of_subpool']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-28 10:13:36 - Second BRF - no BAL multiplier...\n",
      "2021-01-28 10:15:13 - Done\n",
      "2021-01-28 10:15:13 - Second BRF - with temp BAL multiplier (3)...\n",
      "2021-01-28 10:16:56 - Done\n"
     ]
    }
   ],
   "source": [
    "TEMP_BAL_MULTIPLIER = 3\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Second BRF - no BAL multiplier...')\n",
    "\n",
    "brf = subpools['norm_weights']. \\\n",
    "                groupby(['address','block_number','shareholders_subpool']). \\\n",
    "                agg(get_BRF)\n",
    "brf.name = 'second_pass_brf_mult1'\n",
    "subpools = subpools.join(brf)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Done')\n",
    "\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + f' - Second BRF - with temp BAL multiplier ({TEMP_BAL_MULTIPLIER})...')\n",
    "brf = subpools['norm_weights']. \\\n",
    "                groupby(['address','block_number','shareholders_subpool']). \\\n",
    "                agg(get_BRF, bal_multiplier = TEMP_BAL_MULTIPLIER)\n",
    "brf.name = 'second_pass_brf_with_temp_mult'\n",
    "subpools = subpools.join(brf)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "subpools['adjustedLiquidityPreStaking'] = subpools['token_capped_usd_balance'] * \\\n",
    "                                            subpools['fee_factor'] * \\\n",
    "                                            subpools['wrap_factor'] * \\\n",
    "                                            subpools['second_pass_brf_mult1']\n",
    "\n",
    "subpools['adjustedLiquidityWithTempStakingMult'] = subpools['token_capped_usd_balance'] * \\\n",
    "                                            subpools['fee_factor'] * \\\n",
    "                                            subpools['wrap_factor'] * \\\n",
    "                                            subpools['second_pass_brf_with_temp_mult']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute final BAL multiplier\n",
    "total_adjustedLiquidityPreStaking = subpools['adjustedLiquidityPreStaking'].groupby('block_number').sum()\n",
    "total_adjustedLiquidityWithStakingTempMult = subpools['adjustedLiquidityWithTempStakingMult'].groupby('block_number').sum()\n",
    "final_desired_adjusted_liquidity = total_adjustedLiquidityPreStaking / (1-STAKERS_SHARE)\n",
    "stretch = (final_desired_adjusted_liquidity - total_adjustedLiquidityPreStaking) / \\\n",
    "            (total_adjustedLiquidityWithStakingTempMult - total_adjustedLiquidityPreStaking)\n",
    "final_bal_multiplier = 1 + stretch * (TEMP_BAL_MULTIPLIER - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-28 10:16:56 - Third BRF - with final BAL multiplier...\n",
      "2021-01-28 10:19:03 - Done\n"
     ]
    }
   ],
   "source": [
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Third BRF - with final BAL multiplier...')\n",
    "brf = subpools['norm_weights']. \\\n",
    "                groupby(['address','block_number','shareholders_subpool']). \\\n",
    "                agg(get_BRF, bal_multiplier = final_bal_multiplier)\n",
    "brf.name = 'third_pass_brf_with_final_mult'\n",
    "subpools = subpools.join(brf)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the final adjusted liquidity of each token in each subpool at each block\n",
    "subpools['finalAdjustedLiquidity'] = subpools['token_capped_usd_balance'] * \\\n",
    "                                            subpools['fee_factor'] * \\\n",
    "                                            subpools['wrap_factor'] * \\\n",
    "                                            subpools['third_pass_brf_with_final_mult']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the total final adjusted liquidity at each block\n",
    "total_final_adjustedLiquidity = subpools['finalAdjustedLiquidity'].groupby('block_number').sum()\n",
    "\n",
    "# compute the share of liquidity provided by each token in each subpool\n",
    "share_of_liquidity = subpools['finalAdjustedLiquidity'] / total_final_adjustedLiquidity\n",
    "share_of_liquidity.name = 'share_of_liquidity'\n",
    "subpools = subpools.join(share_of_liquidity)\n",
    "\n",
    "# compute the BAL mined by each token in each subpool at each block, proportional to the share of liquidity\n",
    "subpools['BAL_mined'] = subpools['share_of_liquidity'] * WEEKLY_MINED / len(snapshot_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the BAL mined by each LP proportional to their share of the pool\n",
    "bal_mined_by_subpool_per_block = subpools['BAL_mined']. \\\n",
    "                                    groupby(['address','block_number','shareholders_subpool']). \\\n",
    "                                    sum()\n",
    "\n",
    "total_bpt = bpt_balances['bpt'].groupby(['address','block_number','shareholders_subpool']).sum()\n",
    "share_of_pool = bpt_balances['bpt'] / total_bpt\n",
    "\n",
    "bal_mined = bpt_balances.copy()\n",
    "bal_mined['bal_mined'] = (bal_mined_by_subpool_per_block * share_of_pool)\n",
    "bal_mined.reset_index(inplace=True)\n",
    "chksums = {x: Web3.toChecksumAddress(x) for x in bal_mined['bpt_holder'].drop_duplicates()}\n",
    "bal_mined['chksum_bpt_holder'] = bal_mined['bpt_holder'].apply(lambda x: chksums[x])\n",
    "bal_mined.set_index(['address', 'block_number', 'shareholders_subpool', 'chksum_bpt_holder'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals = bal_mined['bal_mined'].groupby('chksum_bpt_holder').sum()\n",
    "\n",
    "if not REALTIME_ESTIMATOR:\n",
    "    totals[totals>=CLAIM_THRESHOLD].apply(lambda x: format(x, f'.{CLAIM_PRECISION}f')).to_json(reports_dir+'/_totalsPreRedirect.json',\n",
    "                                                  indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not REALTIME_ESTIMATOR:\n",
    "    # save pools time series\n",
    "    mined_by_pools = subpools['BAL_mined'].groupby(['address','block_number']).sum()\n",
    "    mined_by_pools = mined_by_pools[mined_by_pools>=CLAIM_THRESHOLD].apply(lambda x: format(x, f'.{CLAIM_PRECISION}f'))\n",
    "    mined_by_pools = pd.DataFrame(mined_by_pools).reset_index()\n",
    "    mined_by_pools = mined_by_pools.pivot(index='address', columns='block_number', values='BAL_mined')\n",
    "    mined_by_pools.to_json(reports_dir+'/_poolsSeries.json.zip',\n",
    "                           orient='index',\n",
    "                           indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redirect and Redistribute\n",
    "Recursively:\n",
    "* redirects BAL earned by one address to another\n",
    "* redistributes BAL earned by a smart contract to its token holders\n",
    "  * a smart contract can earn BAL if it is the controller of a private pool (eg smart pools) or if it holds BPT of finalized pools (eg staking contracts)\n",
    "  * by doing this recursively we also account for staking contracts that hold BPTs of smart pools (BAL earned by the CRP is redistributed to its token holders; then the subset of BAL that goes to the staking contract is redistributed to its holders)\n",
    "  * all CRPs created via the CRPFactory are redistributers by default. Other contracts can PR into `config/redistribute.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get addresses that redirect\n",
    "if REALTIME_ESTIMATOR:\n",
    "    url = 'https://raw.githubusercontent.com/balancer-labs/bal-mining-scripts/master/config/redirect.json'\n",
    "    jsonurl = urlopen(url)\n",
    "    redirects = json.loads(jsonurl.read())\n",
    "else:\n",
    "    redirects = json.load(open('config/redirect.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get addresses that redistribute\n",
    "if REALTIME_ESTIMATOR:\n",
    "    url = 'https://raw.githubusercontent.com/balancer-labs/bal-mining-scripts/master/config/redistribute.json'\n",
    "    jsonurl = urlopen(url)\n",
    "    redistributers_dict = json.loads(jsonurl.read())\n",
    "else:\n",
    "    redistributers_dict = json.load(open('config/redistribute.json'))\n",
    "redistributers_list = list(redistributers_dict.keys())\n",
    "# get list of CRPs\n",
    "sql = 'SELECT pool FROM `blockchain-etl.ethereum_balancer.CRPFactory_event_LogNewCrp`'\n",
    "# Requires setting the environment variable GOOGLE_APPLICATION_CREDENTIALS \n",
    "# to the file path of the JSON file that contains a service account key \n",
    "# with access to the token_balances_subset view\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "bqstorageclient = bigquery_storage.BigQueryReadClient()\n",
    "crps = (\n",
    "    client.query(sql)\n",
    "    .result()\n",
    "    .to_dataframe(bqstorage_client=bqstorageclient)\n",
    ")\n",
    "\n",
    "redistributers_list.extend(crps['pool'].drop_duplicates().apply(Web3.toChecksumAddress))\n",
    "# print('Redistributers: {}'.format(redistributers_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-28 10:19:26 - Querying Bigquery for the token holders of the redistributers ...\n",
      "2021-01-28 10:19:50 - Done!\n"
     ]
    }
   ],
   "source": [
    "# get redistributers' token holders\n",
    "sql = \"\"\"\n",
    "select * from `blockchain-etl.ethereum_balancer.view_token_balances_subset`\n",
    "where token_address in ({})\n",
    "and block_number in ({})\n",
    "and balance > 0\n",
    "\"\"\".format('\\''+'\\',\\''.join(map(lambda x: x.lower(), redistributers_list))+'\\'', \n",
    "           ','.join(snapshot_blocks_as_str))\n",
    "# print(sql)\n",
    "\n",
    "# Requires setting the environment variable GOOGLE_APPLICATION_CREDENTIALS \n",
    "# to the file path of the JSON file that contains a service account key \n",
    "\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Querying Bigquery for the token holders of the redistributers ...')\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "bqstorageclient = bigquery_storage.BigQueryReadClient()\n",
    "running_balances = (\n",
    "    client.query(sql)\n",
    "    .result()\n",
    "    .to_dataframe(bqstorage_client=bqstorageclient)\n",
    ")\n",
    "running_balances['balance'] = running_balances['balance'].astype(float)\n",
    "running_balances['address'] = running_balances['address'].apply(Web3.toChecksumAddress)\n",
    "running_balances['token_address'] = running_balances['token_address'].apply(Web3.toChecksumAddress)\n",
    "running_balances = running_balances.rename(columns={\"token_address\": \"redistributer\", \"address\": \"share_holder\"})\n",
    "running_balances.set_index(['block_number','redistributer','share_holder'], inplace=True)\n",
    "\n",
    "shares = pd.DataFrame(running_balances['balance'])/pd.DataFrame(running_balances.groupby(['block_number','redistributer']).sum()['balance'])\n",
    "shares.columns = ['perc_share']\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redirect #1: 19 redirectors found\n",
      "Redistribute #1: 37 Redistributors found\n"
     ]
    }
   ],
   "source": [
    "miners = bal_mined['bal_mined'].groupby(['block_number', 'chksum_bpt_holder']).sum().reset_index()\n",
    "miners = miners[miners['bal_mined']>0]\n",
    "miners.rename(columns={'chksum_bpt_holder':'miner'}, inplace=True)\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    n = len(miners['miner'].drop_duplicates()[miners['miner'].drop_duplicates().isin(redirects.keys())])\n",
    "    print(f'Redirect #{i+1}: {n} redirectors found')\n",
    "    # redirect\n",
    "    miners['miner'] = miners['miner'].apply(lambda x: redirects.get(x,x))\n",
    "\n",
    "    n = len(miners['miner'].drop_duplicates()[miners['miner'].drop_duplicates().isin(redistributers_list)])\n",
    "    print(f'Redistribute #{i+1}: {n} Redistributors found')\n",
    "    # redistribute\n",
    "    # first assume all miners are redistributing\n",
    "    miners.rename(columns={'miner':'redistributer'}, inplace=True)\n",
    "    miners.set_index(['block_number', 'redistributer'], inplace=True)\n",
    "    # join with shares of redistributing contracts\n",
    "    miners = miners.join(shares, how='left').reset_index()\n",
    "    # miners are the shareholders of the redistributing contracts; or the original redistributer if NA\n",
    "    miners['miner'] = miners['share_holder'].fillna(miners['redistributer'])\n",
    "    # the share of BAL for each miner is the share of the redistribution contract they own; or 1 if NA\n",
    "    miners['perc_share'] = miners['perc_share'].fillna(1)\n",
    "    # compute BAL earned by each miner\n",
    "    miners['bal_mined'] = miners['bal_mined'] * miners['perc_share']\n",
    "    # at this point, same miner might earn BAL from different sources, so we need to aggregate again\n",
    "    miners = miners[['block_number', 'miner', 'bal_mined']].groupby(['block_number', 'miner']).sum().reset_index()\n",
    "    \n",
    "    redirecters_remain = miners['miner'].drop_duplicates().isin(redirects.keys()).any()\n",
    "    redistributers_remain = miners['miner'].drop_duplicates().isin(redistributers_list).any()\n",
    "    if not redirecters_remain and not redistributers_remain:\n",
    "        break\n",
    "\n",
    "totals = miners[['miner','bal_mined']].groupby('miner').sum()['bal_mined']\n",
    "\n",
    "if not REALTIME_ESTIMATOR:\n",
    "    if gov_factor > 1:\n",
    "        filename = '/_totalsPreGovFactor.json'\n",
    "    else:\n",
    "        filename = '/_totalsLiquidityMining.json'\n",
    "\n",
    "    totals[totals>=CLAIM_THRESHOLD].apply(lambda x: \\\n",
    "                                          format(x, f'.{CLAIM_PRECISION}f')).\\\n",
    "                                            to_json(reports_dir+filename,\n",
    "                                                    indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gov Factor\n",
    "Liquidity providers that participate in the governance of Balancer get a bonus on the BAL earned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249 addresses voted on proposal QmRVELW33aicrnyL16EM18spDJeRzzpM6NZv469XxAXffx\n",
      "0 addresses voted through delegators\n",
      "281 addresses voted on proposal QmfJWL3Di2CCzWcH1A3YCDsmZhxn3cgeiavBnNWEeopzAg\n",
      "0 addresses voted through delegators\n",
      "232 addresses voted on proposal QmXQpKyw1BvYgZtvC2KGqrDezWBfSUBEKK77Kx866yBLYf\n",
      "0 addresses voted through delegators\n",
      "343 total unique voters\n",
      "govFactor expansion: 1.03\n",
      "BAL post-govFactor: 73187.000000000029103830\n"
     ]
    }
   ],
   "source": [
    "# apply govFactor\n",
    "if gov_factor > 1:\n",
    "    voters = []\n",
    "    for i,p in latest_gov_proposals.iterrows():\n",
    "        voters_url = f'https://hub.snapshot.page/api/balancer/proposal/{i}'\n",
    "        prop_voters = list(json.loads(requests.get(voters_url).content).keys())\n",
    "        print(f'{len(prop_voters)} addresses voted on proposal {i}')\n",
    "        voters.extend(prop_voters)\n",
    "#     latest_gov_proposal_snapshot_block = proposals.loc[latest_gov_proposal,\n",
    "\n",
    "        # get delegators and delegatees\n",
    "        url = 'https://api.thegraph.com/subgraphs/name/snapshot-labs/snapshot'\n",
    "        query = f'''query {{\n",
    "          delegations(block: {{number: {p['msg']['payload'].get('snapshot',0)}}}, \n",
    "              first: 1000, \n",
    "              where: {{space_in: [\"balancer\", \"\"]}}) {{\n",
    "                delegate\n",
    "                delegator\n",
    "          }}\n",
    "        }}'''\n",
    "        r = requests.post(url, json = {'query':query})\n",
    "        delegations = pd.DataFrame(json.loads(r.content)['data']['delegations'])\n",
    "        if len(delegations)==1000:\n",
    "                warnings.warn('Delegations reached 1000, implement pagination')\n",
    "        delegators_that_voted_indirectly = delegations[delegations.delegate.isin(voters)]['delegator']\n",
    "        print(f'{len(delegators_that_voted_indirectly)} addresses voted through delegators')\n",
    "        voters.extend(delegators_that_voted_indirectly)\n",
    "    \n",
    "    voters = list(dict.fromkeys(voters)) #drop duplicates\n",
    "    print(f'{len(voters)} total unique voters')\n",
    "    \n",
    "    totals_pre_govfactor = totals.copy()\n",
    "    totals[totals.index.isin(voters)] = totals * gov_factor\n",
    "    \n",
    "    expanded_BAL_mined = totals.sum()\n",
    "    \n",
    "    totals = totals * WEEKLY_MINED / expanded_BAL_mined\n",
    "    totals_post_govfactor = totals.copy()\n",
    "\n",
    "    print('govFactor expansion: {:.2f}'.format(expanded_BAL_mined/WEEKLY_MINED))\n",
    "    print('BAL post-govFactor: {:.18f}'.format(totals.sum()))\n",
    "    \n",
    "    if not REALTIME_ESTIMATOR:\n",
    "        filename = '/_totalsLiquidityMining.json'\n",
    "        totals[totals>=CLAIM_THRESHOLD].apply(lambda x: \\\n",
    "                                              format(x, f'.{CLAIM_PRECISION}f')).\\\n",
    "                                                to_json(reports_dir+filename,\n",
    "                                                        indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-28 10:20:17\n"
     ]
    }
   ],
   "source": [
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update real time estimates in GBQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:07,  7.07s/it]\n",
      "1it [00:08,  8.40s/it]\n"
     ]
    }
   ],
   "source": [
    "if REALTIME_ESTIMATOR:\n",
    "    # delete this week's previous estimates\n",
    "    project_id = os.environ['GCP_PROJECT']\n",
    "    sql = f'''\n",
    "        DELETE FROM {project_id}.bal_mining_estimates.pool_estimates\n",
    "        WHERE week = {WEEK}\n",
    "    '''\n",
    "    client = bigquery.Client()\n",
    "    query = client.query(sql)\n",
    "    query.result();\n",
    "    sql = f'''\n",
    "        DELETE FROM {project_id}.bal_mining_estimates.lp_estimates\n",
    "        WHERE week = {WEEK}\n",
    "    '''\n",
    "    client = bigquery.Client()\n",
    "    query = client.query(sql)\n",
    "    query.result();\n",
    "\n",
    "    # write to GBQ (LPs)\n",
    "    cur_estimate = pd.DataFrame(totals)\n",
    "    cur_estimate.columns = ['earned']\n",
    "    cur_estimate.index.name = 'address'\n",
    "    \n",
    "    try:\n",
    "        prev_estimate = pd.read_gbq('select address, earned, timestamp from bal_mining_estimates.lp_estimates', \n",
    "                        project_id=os.environ['GCP_PROJECT'])\n",
    "        prev_estimate.set_index('address', inplace=True)\n",
    "        prev_estimate_timestamp = prev_estimate.loc[0, 'timestamp']\n",
    "    except:\n",
    "        prev_estimate_timestamp = 0\n",
    "    if prev_estimate_timestamp < start_block_timestamp:\n",
    "        #previous estimate is last week's; compute velocity from end_block_timestamp and start_block_timestamp\n",
    "        delta_t = (end_block_timestamp - start_block_timestamp)\n",
    "        earned = cur_estimate['earned'].astype(float)\n",
    "        cur_estimate['velocity'] = (earned/delta_t).apply(lambda x: format(x, f'.{18}f'))\n",
    "    else:\n",
    "        #compute velocity based on increase and time passed\n",
    "        delta_t = (end_block_timestamp - prev_estimate_timestamp)\n",
    "        diff_estimate = cur_estimate.join(prev_estimate, rsuffix='_prev').fillna(0)\n",
    "        cur_earned = diff_estimate['earned'].astype(float)\n",
    "        prev_earned = diff_estimate['earned_prev'].astype(float)\n",
    "        cur_estimate['velocity'] = ((cur_earned-prev_earned)/delta_t).apply(lambda x: format(x, f'.{18}f'))\n",
    "        \n",
    "    cur_estimate['earned'] = cur_estimate['earned'].apply(lambda x: format(x, f'.{18}f'))\n",
    "    cur_estimate['timestamp'] = end_block_timestamp\n",
    "    cur_estimate['week'] = WEEK\n",
    "    cur_estimate.reset_index(inplace=True)\n",
    "    cur_estimate.to_gbq( 'bal_mining_estimates.lp_estimates', \n",
    "                        project_id=os.environ['GCP_PROJECT'], \n",
    "                        if_exists='append')\n",
    "\n",
    "    # write to GBQ (pools)\n",
    "    cur_estimate = pd.DataFrame(subpools['BAL_mined'].groupby('address').sum())\n",
    "    cur_estimate.columns = ['earned']\n",
    "    cur_estimate['earned'] = cur_estimate['earned'].apply(lambda x: format(x, f'.{18}f'))\n",
    "    cur_estimate.index.name = 'address'\n",
    "    \n",
    "    try:\n",
    "        prev_estimate = pd.read_gbq('select address, earned, timestamp from bal_mining_estimates.pool_estimates', \n",
    "                        project_id=os.environ['GCP_PROJECT'])\n",
    "        prev_estimate.set_index('address', inplace=True)\n",
    "        prev_estimate_timestamp = prev_estimate.loc[0, 'timestamp']\n",
    "    except:\n",
    "        prev_estimate_timestamp = 0\n",
    "    if prev_estimate_timestamp < start_block_timestamp:\n",
    "        #previous estimate is last week's; compute velocity from end_block_timestamp and start_block_timestamp\n",
    "        delta_t = (end_block_timestamp - start_block_timestamp)\n",
    "        earned = cur_estimate['earned'].astype(float)\n",
    "        cur_estimate['velocity'] = (earned/delta_t).apply(lambda x: format(x, f'.{18}f'))\n",
    "    else:\n",
    "        #compute velocity based on increase and time passed\n",
    "        delta_t = (end_block_timestamp - prev_estimate_timestamp)\n",
    "        diff_estimate = cur_estimate.join(prev_estimate, rsuffix='_prev').fillna(0)\n",
    "        cur_earned = diff_estimate['earned'].astype(float)\n",
    "        prev_earned = diff_estimate['earned_prev'].astype(float)\n",
    "        cur_estimate['velocity'] = ((cur_earned-prev_earned)/delta_t).apply(lambda x: format(x, f'.{18}f'))\n",
    "        \n",
    "    cur_estimate['timestamp'] = end_block_timestamp\n",
    "    cur_estimate['week'] = WEEK\n",
    "    cur_estimate.reset_index(inplace=True)\n",
    "    cur_estimate.to_gbq( 'bal_mining_estimates.pool_estimates', \n",
    "                        project_id=os.environ['GCP_PROJECT'], \n",
    "                        if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gas Reimbursement Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not REALTIME_ESTIMATOR:\n",
    "    print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Querying Bigquery for eligible swaps and reimbursement values ...')\n",
    "\n",
    "    sql = '''\n",
    "    WITH n_swaps as (\n",
    "        SELECT\n",
    "            txns.`block_number`,\n",
    "            txns.`block_timestamp`,\n",
    "            txns.`from_address`,\n",
    "            txns.`hash`,\n",
    "            txns.`receipt_gas_used`,\n",
    "            txns.`gas_price`,\n",
    "            count(1) as n_swaps\n",
    "        FROM `blockchain-etl.ethereum_balancer.BPool_event_LOG_SWAP` swaps\n",
    "        INNER JOIN `bigquery-public-data.crypto_ethereum.transactions` txns\n",
    "        ON swaps.transaction_hash = txns.`hash`\n",
    "        WHERE 1=1\n",
    "        AND swaps.block_timestamp >= TIMESTAMP_SECONDS({0})\n",
    "        AND txns.block_timestamp >= TIMESTAMP_SECONDS({0})\n",
    "        AND swaps.block_timestamp < TIMESTAMP_SECONDS({1})\n",
    "        AND txns.block_timestamp < TIMESTAMP_SECONDS({1})\n",
    "        AND txns.to_address = '0x3e66b66fd1d0b02fda6c811da9e0547970db2f21'\n",
    "        AND swaps.tokenIn IN ('0xc02aaa39b223fe8d0a0e5c4f27ead9083c756cc2',\n",
    "                                 '0xa0b86991c6218b36c1d19d4a2e9eb0ce3606eb48',\n",
    "                                 '0x6b175474e89094c44da98b954eedeac495271d0f',\n",
    "                                 '0xba100000625a3754423978a60c9317c58a424e3d',\n",
    "                                 '0x2260fac5e5542a773aa44fbcfedf7c193bc2c599')\n",
    "        AND swaps.tokenOut IN ('0xc02aaa39b223fe8d0a0e5c4f27ead9083c756cc2',\n",
    "                                 '0xa0b86991c6218b36c1d19d4a2e9eb0ce3606eb48',\n",
    "                                 '0x6b175474e89094c44da98b954eedeac495271d0f',\n",
    "                                 '0xba100000625a3754423978a60c9317c58a424e3d',\n",
    "                                 '0x2260fac5e5542a773aa44fbcfedf7c193bc2c599')\n",
    "        GROUP BY 1,2,3,4,5,6\n",
    "    ),\n",
    "    median_gas_prices AS (\n",
    "        SELECT DISTINCT\n",
    "            txns.`block_number`,\n",
    "            PERCENTILE_CONT(txns.`gas_price`, 0.5) OVER(PARTITION BY txns.`block_number`) AS block_median_gas_price\n",
    "        FROM `bigquery-public-data.crypto_ethereum.transactions` txns\n",
    "        INNER JOIN n_swaps ON txns.block_number = n_swaps.block_number\n",
    "        WHERE 1=1\n",
    "        AND txns.block_timestamp >= TIMESTAMP_SECONDS({0})\n",
    "        AND txns.block_timestamp < TIMESTAMP_SECONDS({1})\n",
    "    ),\n",
    "    reimbursements AS (\n",
    "        SELECT n.*, m.block_median_gas_price,\n",
    "        CASE WHEN receipt_gas_used > n_swaps * 100000 THEN n_swaps * 100000 ELSE receipt_gas_used END AS gas_reimbursement,\n",
    "        CASE WHEN gas_price > block_median_gas_price THEN block_median_gas_price ELSE gas_price END AS reimbursement_price\n",
    "        FROM n_swaps n \n",
    "        INNER JOIN median_gas_prices m\n",
    "        ON n.block_number = m.block_number\n",
    "    )\n",
    "\n",
    "    SELECT\n",
    "        from_address as address,\n",
    "        SUM(gas_reimbursement*reimbursement_price)/1E18 as eth_reimbursement \n",
    "    FROM reimbursements \n",
    "    GROUP BY 1\n",
    "    ORDER BY 1\n",
    "    '''.format(start_block_timestamp, end_block_timestamp)\n",
    "\n",
    "\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    bqstorageclient = bigquery_storage.BigQueryReadClient()\n",
    "    reimbursements = (\n",
    "        client.query(sql)\n",
    "        .result()\n",
    "        .to_dataframe(bqstorage_client=bqstorageclient)\n",
    "    )\n",
    "    print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Done!')\n",
    "    print(f'ETH reimbursements for the week: {sum(reimbursements.eth_reimbursement)}')\n",
    "\n",
    "    # get BAL:ETH price feed from Coingecko\n",
    "    bal_eth_coingecko = 'https://api.coingecko.com/api/v3/coins/ethereum/contract/0xba100000625a3754423978a60c9317c58a424e3d/market_chart/range?vs_currency=eth&from={0}&to={1}'.format(start_block_timestamp, end_block_timestamp)\n",
    "\n",
    "    baleth_feed = pd.read_json(bal_eth_coingecko)['prices']\n",
    "    baleth_feed = pd.DataFrame(baleth_feed.tolist(), index=baleth_feed.index, columns=['timestamp','price'])\n",
    "    baleth_feed['datetime'] = pd.to_datetime(baleth_feed['timestamp']/1000, unit='s')\n",
    "    baleth_feed.drop(columns=['timestamp'], inplace=True)\n",
    "    baleth_feed.set_index('datetime', inplace=True)\n",
    "    baleth_feed.plot(title='BAL:ETH');\n",
    "    plt.show()\n",
    "    print(f'Median BAL:ETH price for the week: {np.median(baleth_feed)}')\n",
    "    reimbursements['bal_reimbursement'] = reimbursements['eth_reimbursement'] / np.median(baleth_feed)\n",
    "    reimbursements['address'] = reimbursements['address'].apply(Web3.toChecksumAddress)\n",
    "    reimbursements.set_index('address', inplace=True)\n",
    "    reimbursements = reimbursements['bal_reimbursement']\n",
    "    reimbursements[reimbursements>=CLAIM_THRESHOLD].apply(\\\n",
    "       lambda x: format(x, f'.{CLAIM_PRECISION}f')).to_json(reports_dir+'/_gasReimbursement.json',\n",
    "       indent=4)\n",
    "    print(f'BAL reimbursements for the week: {sum(reimbursements)}')\n",
    "\n",
    "    # combine BAL from liquidity mining and gas reimbursements\n",
    "    totals = pd.DataFrame(totals).join(reimbursements, how='outer')\n",
    "    totals.fillna(0, inplace=True)\n",
    "    totals['bal_total'] = totals['bal_mined'] + totals['bal_reimbursement']\n",
    "    totals = totals['bal_total']\n",
    "    totals[totals>=CLAIM_THRESHOLD].apply(\\\n",
    "       lambda x: format(x, f'.{CLAIM_PRECISION}f')).to_json(reports_dir+'/_totals.json',\n",
    "       indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 tokens:\n",
      "0xc02aaa39b223fe8d0a0e5c4f27ead9083c756cc2\n",
      "0xba100000625a3754423978a60c9317c58a424e3d\n",
      "0x2260fac5e5542a773aa44fbcfedf7c193bc2c599\n",
      "0xa0b86991c6218b36c1d19d4a2e9eb0ce3606eb48\n",
      "0x6b175474e89094c44da98b954eedeac495271d0f\n",
      "0x9f8f72aa9304c8b593d555f12ef6589cc3a579a2\n",
      "0x6810e776880c02933d47db1b9fc05908e5386b96\n",
      "0xbc396689893d065f41bc2c6ecbee5e0085233447\n",
      "0x7fc66500c84a76ad7e9c93437bfc5ac33e2ddae9\n",
      "0x04fa0d235c4abf4bcf4787af4cf447de572ef828\n"
     ]
    }
   ],
   "source": [
    "top_tokens = subpools['BAL_mined'].groupby(['token_address']).sum().sort_values(ascending=False).head(10).index\n",
    "subpools['datetime'] = pd.to_datetime(subpools.timestamp, unit='s')\n",
    "rewards_per_token = subpools.groupby(['token_address','datetime']).sum()['BAL_mined']\n",
    "print('Top 10 tokens:\\n' + '\\n'.join(top_tokens))\n",
    "if not REALTIME_ESTIMATOR:\n",
    "    ax = pd.DataFrame(rewards_per_token).reset_index().\\\n",
    "        pivot(index='datetime', columns='token_address', values='BAL_mined')[top_tokens].\\\n",
    "        plot(figsize = (15,10),\n",
    "             title = 'BAL mined by top 10 tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 pools:\n",
      "0x59a19d8c652fa0284f44113d0ff9aba70bd46fb4\n",
      "0x1eff8af5d577060ba4ac8a29a13525bb0ee2a3d5\n",
      "0x9dde0b1d39d0d2c6589cde1bfed3542d2a3c5b11\n",
      "0x8b6e6e7b5b3801fed2cafd4b22b8a16c2f2db21a\n",
      "0xe867be952ee17d2d294f2de62b13b9f4af521e9a\n",
      "0x221bf20c2ad9e5d7ec8a9d1991d8e2edcfcb9d6c\n",
      "0x6b9887422e2a4ae11577f59ea9c01a6c998752e2\n",
      "0xe93e8aa4e88359dacf33c491cf5bd56eb6c110c1\n",
      "0xf54025af2dc86809be1153c1f20d77adb7e8ecf4\n",
      "0xba20d4f41121b997a1eaca6d938ac40b67dad226\n"
     ]
    }
   ],
   "source": [
    "rewards_per_pool = subpools.groupby(['address','datetime']).sum()['BAL_mined']\n",
    "top_pools = subpools['BAL_mined'].groupby(['address']).sum().sort_values(ascending=False).head(10).index\n",
    "print('Top 10 pools:\\n' + '\\n'.join(top_pools))\n",
    "if not REALTIME_ESTIMATOR:\n",
    "    ax = pd.DataFrame(rewards_per_pool).reset_index().\\\n",
    "        pivot(index='datetime', columns='address', values='BAL_mined')[top_pools].\\\n",
    "        plot(figsize = (15,10),\n",
    "             title = 'BAL earned by top 10 pools')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 LPs:\n",
      "0xAfC2F2D803479A2AF3A72022D54cc0901a0ec0d6\n",
      "0x49a2DcC237a65Cc1F412ed47E0594602f6141936\n",
      "0xD85A7A3C5F08E3E709c233E133cE1335fBbF5518\n",
      "0x6b7Ac46d09d2ADF4CeBe2995EbF9d97E13E9E257\n",
      "0xBc79855178842FDBA0c353494895DEEf509E26bB\n",
      "0x97D4B02Ce33C399fFeC618Bfd2D5Bf7108e556ac\n",
      "0x4f58985B75EeC8f14C536878A19EAdF4a1960D6c\n",
      "0xfF052381092420B7F24cc97FDEd9C0c17b2cbbB9\n",
      "0x4a49985B14bD0ce42c25eFde5d8c379a48AB02F3\n",
      "0xCeCD5557FCC879BfeB4d209eAf646311fC30eFd5\n"
     ]
    }
   ],
   "source": [
    "rewards_per_lp = bal_mined['bal_mined'].groupby(['chksum_bpt_holder','block_number']).sum()\n",
    "top_lps = bal_mined['bal_mined'].groupby(['chksum_bpt_holder']).sum().sort_values(ascending=False).head(10).index\n",
    "print('Top 10 LPs:\\n' + '\\n'.join(top_lps))\n",
    "if not REALTIME_ESTIMATOR:\n",
    "    ax = pd.DataFrame(rewards_per_lp).reset_index().\\\n",
    "        pivot(index='block_number', columns='chksum_bpt_holder', values='bal_mined')[top_lps].\\\n",
    "        plot(figsize = (15,10),\n",
    "             title = 'BAL earned by top 10 liquidity providers')\n",
    "    ax.ticklabel_format(axis='x', style='plain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not REALTIME_ESTIMATOR:\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    fig, axs = plt.subplots(ncols=3, nrows=3, figsize=(15, 15))\n",
    "\n",
    "    i = 0\n",
    "    areaplot = subpools.groupby(['datetime','address']).sum().\\\n",
    "                reset_index().pivot(index='datetime', columns='address', values='usd_balance')\n",
    "    # deterministically color code the regions of the plot for visual inspection between weeks\n",
    "    areaplot = areaplot[list(areaplot.sum().sort_values(ascending=False).index)]\n",
    "    pal = ListedColormap(list(map(lambda x: '#'+x[20:26], list(areaplot.columns))))\n",
    "    areaplot.plot.area(legend=False, ax=axs.flat[i], title='USD balance by pool', colormap=pal)\n",
    "\n",
    "    i += 1\n",
    "    areaplot = subpools.groupby(['datetime','address']).sum().\\\n",
    "                reset_index().pivot(index='datetime', columns='address', values='finalAdjustedLiquidity')\n",
    "    areaplot = areaplot[list(areaplot.sum().sort_values(ascending=False).index)]\n",
    "    pal = ListedColormap(list(map(lambda x: '#'+x[20:26], list(areaplot.columns))))\n",
    "    areaplot.plot.area(legend=False, ax=axs.flat[i], title='Adjusted liquidity by pool', colormap=pal)\n",
    "\n",
    "    i += 1\n",
    "    areaplot = subpools.groupby(['datetime','address']).sum().\\\n",
    "                reset_index().pivot(index='datetime', columns='address', values='BAL_mined')\n",
    "    areaplot = areaplot[list(areaplot.sum().sort_values(ascending=False).index)]\n",
    "    pal = ListedColormap(list(map(lambda x: '#'+x[20:26], list(areaplot.columns))))\n",
    "    areaplot.plot.area(legend=False, ax=axs.flat[i], title='BAL mined by pool', colormap=pal)\n",
    "\n",
    "    i += 1\n",
    "    areaplot = subpools.groupby(['datetime','token_address']).sum().\\\n",
    "                reset_index().pivot(index='datetime', columns='token_address', values='usd_balance')\n",
    "    # deterministically color code the regions of the plot for visual inspection between weeks\n",
    "    areaplot = areaplot[list(areaplot.sum().sort_values(ascending=False).index)]\n",
    "    pal = ListedColormap(list(map(lambda x: '#'+x[20:26], list(areaplot.columns))))\n",
    "    areaplot.plot.area(legend=False, ax=axs.flat[i], title='USD balance by token', colormap=pal)\n",
    "\n",
    "    i += 1\n",
    "    areaplot = subpools.groupby(['datetime','token_address']).sum().\\\n",
    "                reset_index().pivot(index='datetime', columns='token_address', values='finalAdjustedLiquidity')\n",
    "    areaplot = areaplot[list(areaplot.sum().sort_values(ascending=False).index)]\n",
    "    pal = ListedColormap(list(map(lambda x: '#'+x[20:26], list(areaplot.columns))))\n",
    "    areaplot.plot.area(legend=False, ax=axs.flat[i], title='Adjusted liquidity by token', colormap=pal)\n",
    "\n",
    "    i += 1\n",
    "    areaplot = subpools.groupby(['datetime','token_address']).sum().\\\n",
    "                reset_index().pivot(index='datetime', columns='token_address', values='BAL_mined')\n",
    "    areaplot = areaplot[list(areaplot.sum().sort_values(ascending=False).index)]\n",
    "    pal = ListedColormap(list(map(lambda x: '#'+x[20:26], list(areaplot.columns))))\n",
    "    areaplot.plot.area(legend=False, ax=axs.flat[i], title='BAL mined by token', colormap=pal)\n",
    "\n",
    "    i += 3\n",
    "    areaplot = bal_mined.groupby(['block_number','chksum_bpt_holder']).sum().\\\n",
    "                reset_index().pivot(index='block_number', columns='chksum_bpt_holder', values='bal_mined')\n",
    "    areaplot = areaplot[list(areaplot.sum().sort_values(ascending=False).index)]\n",
    "    pal = ListedColormap(list(map(lambda x: '#'+x[20:26], list(areaplot.columns))))\n",
    "    areaplot.plot.area(legend=False, ax=axs.flat[i], title='BAL mined by LP', colormap=pal);\n",
    "    axs.flat[i].ticklabel_format(axis='x', style='plain')\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective govFactor: 1.070 vs 0.973\n",
      "Additional BAL mined by governors: 1427.157\n"
     ]
    }
   ],
   "source": [
    "if gov_factor > 1:\n",
    "    pregov = pd.DataFrame(totals_pre_govfactor)\n",
    "    pregov.columns= ['pregov']\n",
    "    postgov = pd.DataFrame(totals_post_govfactor)\n",
    "    postgov.columns= ['postgov']\n",
    "    compare = pregov.join(postgov)\n",
    "    compare['ratio'] = compare['postgov']/compare['pregov']\n",
    "    compare['voter'] = compare.index.isin(voters)\n",
    "    compare['color'] = compare['voter'].apply(lambda x: 'blue' if x else 'red')\n",
    "    effective_gov_factor = compare[compare['voter']]['ratio'].median()\n",
    "    effective_gov_loss = compare[~compare['voter']]['ratio'].median()\n",
    "    print(f'Effective govFactor: {effective_gov_factor:.3f} vs {effective_gov_loss:.3f}')\n",
    "    absolute_increase = (compare[compare['voter']]['postgov'] - compare[compare['voter']]['pregov']).sum()\n",
    "    print(f'Additional BAL mined by governors: {absolute_increase:.3f}')    \n",
    "    if not REALTIME_ESTIMATOR:\n",
    "        ax = compare[compare['voter']].plot.scatter(x='pregov', y='postgov', \n",
    "                                                    c='color', figsize=(10,10),\n",
    "                                                    label='voted'\n",
    "                                                   )\n",
    "        compare[~compare['voter']].plot.scatter(x='pregov', y='postgov', \n",
    "                                                    c='color', figsize=(10,10),\n",
    "                                                    label='abstained', ax=ax\n",
    "                                                   )\n",
    "\n",
    "        x = compare['pregov']\n",
    "        y = compare['pregov']\n",
    "        ax.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)), \n",
    "                color='grey', linestyle='--',\n",
    "                label = 'if no govFactor')\n",
    "\n",
    "        x = compare[compare['voter']]['pregov']\n",
    "        y = compare[compare['voter']]['postgov']\n",
    "        ax.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)), \n",
    "                color='lightblue', linestyle='--',\n",
    "                label = 'w/ govFactor (voters)')\n",
    "        \n",
    "        x = compare[~compare['voter']]['pregov']\n",
    "        y = compare[~compare['voter']]['postgov']\n",
    "        ax.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)), \n",
    "                color='salmon', linestyle='--',\n",
    "                label = 'w/ govFactor (abstained)')\n",
    "\n",
    "\n",
    "        ax.set_xlabel('BAL earned pre-govFactor')\n",
    "        ax.set_ylabel('BAL earned post-govFactor')\n",
    "        ax.set_title('Effects of govFactor on each liquidity provider')\n",
    "        ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
